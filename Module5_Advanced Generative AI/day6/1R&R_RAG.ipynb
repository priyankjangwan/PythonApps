{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d39eb8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem Statement for Ex9\n",
    "\n",
    "**Use Case Title:**\n",
    "**\"Research Paper Selector using Retrieve-and-Rerank RAG (R\\&R-RAG)\"**\n",
    "\n",
    "**Problem Statement:**\n",
    "In academic research, retrieving the most relevant scholarly papers for a specific topic (e.g., \"few-shot learning techniques\") can be challenging due to the large volume of documents and noisy keyword-based results. A basic semantic search using embeddings is often insufficient in terms of ranking the best-matching results based on fine-grained semantic nuances.\n",
    "\n",
    "To improve the **accuracy and relevance** of retrieved results, this project implements a **hybrid RAG pipeline** using:\n",
    "\n",
    "* A **bi-encoder (SentenceTransformer)** for fast semantic retrieval using FAISS.\n",
    "* A **cross-encoder (MS MARCO TinyBERT)** for reranking the top retrieved results using deeper interaction modeling between query and document.\n",
    "\n",
    "This approach enhances **information retrieval quality** for NLP/NLU-based academic literature search.\n",
    "\n",
    "---\n",
    "\n",
    "### What’s New in Ex9 Compared to Ex8?\n",
    "\n",
    "| Feature           | Ex8 (Hybrid Clause Finder) | Ex9 (Research Paper Reranker)    | What's New in Ex9                            |\n",
    "| ----------------- | -------------------------- | -------------------------------- | -------------------------------------------- |\n",
    "| Input Type        | Legal contract PDFs        | Academic papers in CSV           | CSV-based structured document handling       |\n",
    "| Search Type       | BM25 + FAISS hybrid        | FAISS + CrossEncoder hybrid      | CrossEncoder reranking added                 |\n",
    "| Index             | Semantic + BM25            | Semantic only for initial recall | Uses CrossEncoder for deep reranking         |\n",
    "| LLM               | FLAN-T5 via pipeline       | Not used for generation          | Focus is on ranking, not answering           |\n",
    "| Evaluation Output | Answer + source chunks     | Ranked documents with scores     | Reranked paper results with relevance scores |\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Significance\n",
    "\n",
    "This pattern (R\\&R-RAG) is used in:\n",
    "\n",
    "* Academic search engines like Semantic Scholar, Arxiv-Sanity.\n",
    "* Legal document analysis for ranking contract clauses by importance.\n",
    "* Patent retrieval and systematic literature reviews in NLP pipelines.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e19f425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pycaret 3.3.2 requires scikit-learn>1.4.0, but you have scikit-learn 1.3.0 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install sentence-transformers faiss-cpu pandas tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# SentenceTransformer\n",
    "# This is used to load a bi-encoder model.\n",
    "# A bi-encoder converts a single piece of text (like a paper title + abstract or a user query) into a vector (embedding).\n",
    "# These vectors are then stored in FAISS for fast semantic retrieval.\n",
    "# You use SentenceTransformer(\"all-MiniLM-L6-v2\") to:\n",
    "# Convert all academic papers (title + abstract) into vector form.\n",
    "# Also to convert user query into a vector to find the top-k similar papers.\n",
    "\n",
    "# CrossEncoder\n",
    "# This is used to load a cross-encoder model.\n",
    "# A cross-encoder takes a pair of texts (e.g., query + one paper) and processes them together.\n",
    "# It returns a relevance score between 0 and 1 based on how well the paper matches the query.\n",
    "# You use CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L-6\") to:\n",
    "# -Score each (query, document) pair among the top-k retrieved papers.\n",
    "# -Rerank them based on these scores for better final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55be4f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Few-Shot Learning via Prompt Tuning with LLMs</td>\n",
       "      <td>We propose a prompt-tuning strategy using larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meta-Learning for Efficient Few-Shot Classific...</td>\n",
       "      <td>Meta-learning frameworks have shown promising ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Survey on Transformers in Vision</td>\n",
       "      <td>This paper surveys the use of Transformer arch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contrastive Learning for Representation Learning</td>\n",
       "      <td>We explore contrastive learning approaches tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Scaling Laws in Large Language Models</td>\n",
       "      <td>This work investigates how performance scales ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Efficient Retrieval Techniques for Long Documents</td>\n",
       "      <td>We introduce retrieval mechanisms to efficient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Reinforcement Learning with Human Feedback</td>\n",
       "      <td>We discuss reinforcement learning with human f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>An Empirical Study of LLM Prompt Sensitivity</td>\n",
       "      <td>This empirical study examines how LLMs respond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Self-Supervised Pretraining in NLP</td>\n",
       "      <td>This paper analyzes self-supervised learning o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Benchmarks for Multimodal Understanding</td>\n",
       "      <td>We present a collection of benchmarks and eval...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0      Few-Shot Learning via Prompt Tuning with LLMs   \n",
       "1  Meta-Learning for Efficient Few-Shot Classific...   \n",
       "2                 A Survey on Transformers in Vision   \n",
       "3   Contrastive Learning for Representation Learning   \n",
       "4       Neural Scaling Laws in Large Language Models   \n",
       "5  Efficient Retrieval Techniques for Long Documents   \n",
       "6         Reinforcement Learning with Human Feedback   \n",
       "7       An Empirical Study of LLM Prompt Sensitivity   \n",
       "8                 Self-Supervised Pretraining in NLP   \n",
       "9            Benchmarks for Multimodal Understanding   \n",
       "\n",
       "                                            abstract  \n",
       "0  We propose a prompt-tuning strategy using larg...  \n",
       "1  Meta-learning frameworks have shown promising ...  \n",
       "2  This paper surveys the use of Transformer arch...  \n",
       "3  We explore contrastive learning approaches tha...  \n",
       "4  This work investigates how performance scales ...  \n",
       "5  We introduce retrieval mechanisms to efficient...  \n",
       "6  We discuss reinforcement learning with human f...  \n",
       "7  This empirical study examines how LLMs respond...  \n",
       "8  This paper analyzes self-supervised learning o...  \n",
       "9  We present a collection of benchmarks and eval...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load academic papers dataset\n",
    "# Example format: papers.csv with 'title' and 'abstract'\n",
    "df = pd.read_csv(\"papers.csv\")  # <- Replace with your corpus\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e7cf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Few-Shot Learning via Prompt Tuning with LLMs</td>\n",
       "      <td>We propose a prompt-tuning strategy using larg...</td>\n",
       "      <td>Few-Shot Learning via Prompt Tuning with LLMs....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meta-Learning for Efficient Few-Shot Classific...</td>\n",
       "      <td>Meta-learning frameworks have shown promising ...</td>\n",
       "      <td>Meta-Learning for Efficient Few-Shot Classific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Survey on Transformers in Vision</td>\n",
       "      <td>This paper surveys the use of Transformer arch...</td>\n",
       "      <td>A Survey on Transformers in Vision. This paper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contrastive Learning for Representation Learning</td>\n",
       "      <td>We explore contrastive learning approaches tha...</td>\n",
       "      <td>Contrastive Learning for Representation Learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Scaling Laws in Large Language Models</td>\n",
       "      <td>This work investigates how performance scales ...</td>\n",
       "      <td>Neural Scaling Laws in Large Language Models. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Efficient Retrieval Techniques for Long Documents</td>\n",
       "      <td>We introduce retrieval mechanisms to efficient...</td>\n",
       "      <td>Efficient Retrieval Techniques for Long Docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Reinforcement Learning with Human Feedback</td>\n",
       "      <td>We discuss reinforcement learning with human f...</td>\n",
       "      <td>Reinforcement Learning with Human Feedback. We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>An Empirical Study of LLM Prompt Sensitivity</td>\n",
       "      <td>This empirical study examines how LLMs respond...</td>\n",
       "      <td>An Empirical Study of LLM Prompt Sensitivity. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Self-Supervised Pretraining in NLP</td>\n",
       "      <td>This paper analyzes self-supervised learning o...</td>\n",
       "      <td>Self-Supervised Pretraining in NLP. This paper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Benchmarks for Multimodal Understanding</td>\n",
       "      <td>We present a collection of benchmarks and eval...</td>\n",
       "      <td>Benchmarks for Multimodal Understanding. We pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0      Few-Shot Learning via Prompt Tuning with LLMs   \n",
       "1  Meta-Learning for Efficient Few-Shot Classific...   \n",
       "2                 A Survey on Transformers in Vision   \n",
       "3   Contrastive Learning for Representation Learning   \n",
       "4       Neural Scaling Laws in Large Language Models   \n",
       "5  Efficient Retrieval Techniques for Long Documents   \n",
       "6         Reinforcement Learning with Human Feedback   \n",
       "7       An Empirical Study of LLM Prompt Sensitivity   \n",
       "8                 Self-Supervised Pretraining in NLP   \n",
       "9            Benchmarks for Multimodal Understanding   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We propose a prompt-tuning strategy using larg...   \n",
       "1  Meta-learning frameworks have shown promising ...   \n",
       "2  This paper surveys the use of Transformer arch...   \n",
       "3  We explore contrastive learning approaches tha...   \n",
       "4  This work investigates how performance scales ...   \n",
       "5  We introduce retrieval mechanisms to efficient...   \n",
       "6  We discuss reinforcement learning with human f...   \n",
       "7  This empirical study examines how LLMs respond...   \n",
       "8  This paper analyzes self-supervised learning o...   \n",
       "9  We present a collection of benchmarks and eval...   \n",
       "\n",
       "                                             content  \n",
       "0  Few-Shot Learning via Prompt Tuning with LLMs....  \n",
       "1  Meta-Learning for Efficient Few-Shot Classific...  \n",
       "2  A Survey on Transformers in Vision. This paper...  \n",
       "3  Contrastive Learning for Representation Learni...  \n",
       "4  Neural Scaling Laws in Large Language Models. ...  \n",
       "5  Efficient Retrieval Techniques for Long Docume...  \n",
       "6  Reinforcement Learning with Human Feedback. We...  \n",
       "7  An Empirical Study of LLM Prompt Sensitivity. ...  \n",
       "8  Self-Supervised Pretraining in NLP. This paper...  \n",
       "9  Benchmarks for Multimodal Understanding. We pr...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine title and abstract into a single content field\n",
    "# This is done to create a single text representation of each paper.\n",
    "df['content'] = df['title'] + \". \" + df['abstract']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43e3fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 documents.\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame content to a list of documents\n",
    "# This list will be used for embedding and retrieval.\n",
    "# Each document is a string combining the title and abstract of a paper.\n",
    "documents = df['content'].tolist()\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc7fd70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c522a043944492a7f5806d07e4a6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Embed documents using SentenceTransformer (bi-encoder)\n",
    "bi_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "doc_embeddings = bi_encoder.encode(documents, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e97c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create FAISS index\n",
    "dimension = doc_embeddings[0].shape[0]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(doc_embeddings))\n",
    "print(\"FAISS index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Cross-Encoder for reranking (query-doc pairs)\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L-6\", max_length=512)\n",
    "\n",
    "# We're now using a Cross-Encoder model.\n",
    "# The model will take a pair of inputs:\n",
    "# a. A user query (like \"few-shot learning methods\")\n",
    "# b. A document (like a research paper’s content)\n",
    "# And it will score how relevant the document is to the query.\n",
    "# This helps us rerank the top documents retrieved earlier from FAISS.\n",
    "\n",
    "# cross-encoder/ms-marco-TinyBERT-L-6\n",
    "# A tiny, fast transformer-based cross-encoder model fine-tuned for ranking text pairs (like question + passage). It's made available via the sentence-transformers library.\n",
    "\n",
    "# max_length=512\n",
    "# Only read up to 512 tokens from the combined input (query + document).\n",
    "# This avoids memory issues and truncates long documents smartly.\n",
    "# 512 is a typical limit for many transformer models like BERT and TinyBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4592f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Retrieval + Reranking pipeline\n",
    "# This function will:\n",
    "# First retrieve many candidate papers (top_k × 5),\n",
    "# Then rerank them using a cross-encoder,\n",
    "# And finally return the top_k most relevant papers.\n",
    "\n",
    "def retrieve_and_rerank(query, top_k=10):\n",
    "    # Step 1: Vector search (fast recall)\n",
    "    query_embedding = bi_encoder.encode([query]) # It converts the query into a vector (embedding) using the bi-encoder model.\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k * 5) # It performs semantic search in the FAISS index using the query vector.\n",
    "    initial_results = [(documents[i], df.iloc[i]['title'], df.iloc[i]['abstract']) for i in indices[0]]\n",
    "    # For each of the matched document indices, it extracts:\n",
    "    # >The full content (title + abstract),\n",
    "    # >The title alone,\n",
    "    # >The abstract alone.\n",
    "    # These are stored as tuples in a list called initial_results.\n",
    "\n",
    "    # Step 2: Cross-encoder reranking\n",
    "    rerank_pairs = [[query, doc] for doc, _, _ in initial_results]\n",
    "    scores = cross_encoder.predict(rerank_pairs)\n",
    "    reranked = sorted(zip(scores, initial_results), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return reranked[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8e73843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top results for: What are the latest techniques in few-shot learning?\n",
      "\n",
      "📝 Title: Few-Shot Learning via Prompt Tuning with LLMs\n",
      "📊 Score: 0.9026\n",
      "📄 Abstract: We propose a prompt-tuning strategy using large language models for adapting to few-shot settings in NLP tasks. Our method reduces the need for fine-tuning by leveraging prompt engineering....\n",
      "--------------------------------------------------------------------------------\n",
      "📝 Title: Meta-Learning for Efficient Few-Shot Classification\n",
      "📊 Score: 0.7122\n",
      "📄 Abstract: Meta-learning frameworks have shown promising results in few-shot classification by optimizing the initialization of neural networks across tasks....\n",
      "--------------------------------------------------------------------------------\n",
      "📝 Title: Contrastive Learning for Representation Learning\n",
      "📊 Score: 0.0003\n",
      "📄 Abstract: We explore contrastive learning approaches that learn useful representations by pulling semantically similar instances together and pushing dissimilar ones apart....\n",
      "--------------------------------------------------------------------------------\n",
      "📝 Title: Reinforcement Learning with Human Feedback\n",
      "📊 Score: 0.0002\n",
      "📄 Abstract: We discuss reinforcement learning with human feedback (RLHF), which allows agents to align better with human values through reward modeling....\n",
      "--------------------------------------------------------------------------------\n",
      "📝 Title: Self-Supervised Pretraining in NLP\n",
      "📊 Score: 0.0002\n",
      "📄 Abstract: This paper analyzes self-supervised learning objectives for language modeling and their transferability to downstream tasks....\n",
      "--------------------------------------------------------------------------------\n",
      "📝 Title: Efficient Retrieval Techniques for Long Documents\n",
      "📊 Score: 0.0002\n",
      "📄 Abstract: We introduce retrieval mechanisms to efficiently index and access long documents in QA systems, enabling fast recall without sacrificing semantic coverage....\n",
      "--------------------------------------------------------------------------------\n",
      "📝 Title: An Empirical Study of LLM Prompt Sensitivity\n",
      "📊 Score: 0.0002\n",
      "📄 Abstract: This empirical study examines how LLMs respond to various prompt formulations across tasks like summarization, translation, and classification....\n",
      "--------------------------------------------------------------------------------\n",
      "📝 Title: A Survey on Transformers in Vision\n",
      "📊 Score: 0.0002\n",
      "📄 Abstract: This paper surveys the use of Transformer architectures in computer vision, including ViT, DETR, and Swin Transformers, with benchmarks and comparisons....\n",
      "--------------------------------------------------------------------------------\n",
      "📝 Title: Neural Scaling Laws in Large Language Models\n",
      "📊 Score: 0.0002\n",
      "📄 Abstract: This work investigates how performance scales with model size, dataset size, and compute, providing insights into neural scaling laws....\n",
      "--------------------------------------------------------------------------------\n",
      "📝 Title: Benchmarks for Multimodal Understanding\n",
      "📊 Score: 0.0002\n",
      "📄 Abstract: We present a collection of benchmarks and evaluation frameworks for assessing multimodal understanding across image-text tasks....\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "query = \"What are the latest techniques in few-shot learning?\"\n",
    "results = retrieve_and_rerank(query)\n",
    "\n",
    "print(f\"\\n🔍 Top results for: {query}\\n\")\n",
    "for score, (doc, title, abstract) in results:\n",
    "    print(f\"📝 Title: {title}\")\n",
    "    print(f\"📊 Score: {score:.4f}\")\n",
    "    print(f\"📄 Abstract: {abstract[:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47470f4",
   "metadata": {},
   "source": [
    "# Happy Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
