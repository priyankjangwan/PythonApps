{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Topic 1: Evaluation in Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "---\n",
        "\n",
        "### Concept: Why Evaluate RAG Separately?\n",
        "\n",
        "In Retrieval-Augmented Generation (RAG), model outputs depend not only on prompt quality or fine-tuning, but also on:\n",
        "\n",
        "1. **Retrieval Quality** (Did the retriever fetch relevant context?)\n",
        "2. **Groundedness** (Did the output stay faithful to the context?)\n",
        "3. **Answer Relevance** (Did the answer actually help answer the query?)\n",
        "\n",
        "Traditional metrics like accuracy or BLEU are not enough to evaluate these aspects.\n",
        "\n",
        "---\n",
        "\n",
        "### What We Measure in RAG\n",
        "\n",
        "| Metric                 | What it Measures                                                        |\n",
        "| ---------------------- | ----------------------------------------------------------------------- |\n",
        "| **Groundedness**       | Whether the generated output is strictly based on retrieved content     |\n",
        "| **Faithfulness**       | Whether the answer is factually consistent with context                 |\n",
        "| **Relevance**          | Whether the answer is helpful or matches the intent of the question     |\n",
        "| **Source Attribution** | Whether the answer correctly refers to which document it was drawn from |\n",
        "\n",
        "---\n",
        "\n",
        "### Real-World Significance\n",
        "\n",
        "* In enterprise settings (legal, healthcare, HR), **hallucinations** are unacceptable\n",
        "* Evaluating faithfulness and groundedness helps ensure trust in the system\n",
        "\n",
        "---\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "We want to **evaluate the effectiveness** of a RAG pipeline by analyzing how well the generated answers:\n",
        "\n",
        "* Stick to retrieved context (groundedness)\n",
        "* Are factually correct (faithfulness)\n",
        "* Are relevant to the query (relevance)\n",
        "\n",
        "We assume that:\n",
        "\n",
        "* Retrieval has already happened\n",
        "* LLM outputs are available for a set of questions\n",
        "* We manually score them using simple metrics\n",
        "\n",
        "---\n",
        "\n",
        "## Objectives\n",
        "\n",
        "* Understand how to **quantify quality** of retrieved documents\n",
        "* Manually rate the **generated answers** based on **relevance**, **faithfulness**, and **groundedness**\n",
        "* Automate simple scoring for fast analysis\n",
        "\n",
        "---\n",
        "\n",
        "## Sample Inputs\n",
        "\n",
        "Assume we have 5 questions, their retrieved contexts, and the LLM-generated answers.\n",
        "\n",
        "We will score each answer from 0 to 1 for:\n",
        "\n",
        "* `groundedness_score`\n",
        "* `faithfulness_score`\n",
        "* `relevance_score`\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Output\n",
        "\n",
        "```\n",
        "Example 1\n",
        "Question      : What are the key highlights of the 2024 AFGI Report?\n",
        "Groundedness  : 0.8\n",
        "Faithfulness  : 1.0\n",
        "Relevance     : 1.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Xiz92cGIgU9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define evaluation data\n",
        "data = [\n",
        "    {\n",
        "        \"question\": \"What are the key highlights of the 2024 AFGI Report?\",\n",
        "        \"retrieved_context\": \"The 2024 AFGI Report highlighted improvements in access to financing, digital inclusion, and gender-balanced credit participation.\",\n",
        "        \"generated_answer\": \"The AFGI Report 2024 focuses on financial access and gender inclusion.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What was the public debt level in 2024?\",\n",
        "        \"retrieved_context\": \"Public debt rose by 12% due to post-pandemic recovery spending and inflationary pressures.\",\n",
        "        \"generated_answer\": \"In 2024, public debt increased due to high inflation.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How did rural banking evolve in the last year?\",\n",
        "        \"retrieved_context\": \"Rural banking expanded to over 30,000 villages, supported by fintech partnerships and mobile banking.\",\n",
        "        \"generated_answer\": \"Rural banking saw major digital growth with mobile banking reaching thousands of villages.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Step 2: Manual scoring function\n",
        "def evaluate_rag_outputs(dataset):\n",
        "    results = []\n",
        "    for entry in dataset:\n",
        "        # Dummy logic: simulate manual review\n",
        "        # In real case: Use annotators or GPT-based judgment\n",
        "        g_score = 1 if entry[\"generated_answer\"].lower() in entry[\"retrieved_context\"].lower() else 0.8\n",
        "        f_score = 1 if \"increased\" in entry[\"generated_answer\"].lower() or \"focuses\" in entry[\"generated_answer\"].lower() else 0.6\n",
        "        r_score = 1 if any(word in entry[\"generated_answer\"].lower() for word in [\"financial\", \"banking\", \"debt\"]) else 0.5\n",
        "\n",
        "        results.append({\n",
        "            \"question\": entry[\"question\"],\n",
        "            \"groundedness\": g_score,\n",
        "            \"faithfulness\": f_score,\n",
        "            \"relevance\": r_score,\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# Step 3: Run evaluation\n",
        "evaluation_scores = evaluate_rag_outputs(data)\n",
        "\n",
        "# Step 4: Print results\n",
        "for idx, res in enumerate(evaluation_scores):\n",
        "    print(f\"\\nExample {idx+1}\")\n",
        "    print(f\"Question      : {data[idx]['question']}\")\n",
        "    print(f\"Groundedness  : {res['groundedness']}\")\n",
        "    print(f\"Faithfulness  : {res['faithfulness']}\")\n",
        "    print(f\"Relevance     : {res['relevance']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg_dABr0hPCH",
        "outputId": "d181d923-9dcc-4042-c1a2-e65207b28995"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example 1\n",
            "Question      : What are the key highlights of the 2024 AFGI Report?\n",
            "Groundedness  : 0.8\n",
            "Faithfulness  : 1\n",
            "Relevance     : 1\n",
            "\n",
            "Example 2\n",
            "Question      : What was the public debt level in 2024?\n",
            "Groundedness  : 0.8\n",
            "Faithfulness  : 1\n",
            "Relevance     : 1\n",
            "\n",
            "Example 3\n",
            "Question      : How did rural banking evolve in the last year?\n",
            "Groundedness  : 0.8\n",
            "Faithfulness  : 0.6\n",
            "Relevance     : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Output Breakdown**\n",
        "\n",
        "### **Example 1**\n",
        "\n",
        "**Question**: What are the key highlights of the 2024 AFGI Report?\n",
        "**Generated Answer**: \"The AFGI Report 2024 focuses on financial access and gender inclusion.\"\n",
        "**Context**: Talks about **financing**, **digital inclusion**, **gender-balanced credit**.\n",
        "\n",
        "| Metric       | Score | Reasoning                                            |\n",
        "| ------------ | ----- | ---------------------------------------------------- |\n",
        "| Groundedness | 0.8   | Not a direct phrase match, partial overlap only.     |\n",
        "| Faithfulness | 1.0   | Includes key ideas from context (access, inclusion). |\n",
        "| Relevance    | 1.0   | Fully answers the question on highlights.            |\n",
        "\n",
        "### **Example 2**\n",
        "\n",
        "**Question**: What was the public debt level in 2024?\n",
        "**Generated Answer**: \"In 2024, public debt increased due to high inflation.\"\n",
        "**Context**: \"Public debt rose by 12% due to post-pandemic recovery spending and inflationary pressures.\"\n",
        "\n",
        "| Metric       | Score | Reasoning                                            |\n",
        "| ------------ | ----- | ---------------------------------------------------- |\n",
        "| Groundedness | 0.8   | Partial match; doesn’t mention 12%, omits one cause. |\n",
        "| Faithfulness | 1.0   | Correctly states debt increased due to inflation.    |\n",
        "| Relevance    | 1.0   | Answers the debt-related question directly.          |\n",
        "\n",
        "### **Example 3**\n",
        "\n",
        "**Question**: How did rural banking evolve in the last year?\n",
        "**Generated Answer**: \"Rural banking saw major digital growth with mobile banking reaching thousands of villages.\"\n",
        "**Context**: Describes **30,000 villages**, **fintech partnerships**, and **mobile banking**.\n",
        "\n",
        "| Metric       | Score | Reasoning                                                        |\n",
        "| ------------ | ----- | ---------------------------------------------------------------- |\n",
        "| Groundedness | 0.8   | Close match, but lacks precision (e.g., \"30,000\" not mentioned). |\n",
        "| Faithfulness | 0.6   | Overgeneralized; omits fintech support, exaggerates slightly.    |\n",
        "| Relevance    | 1.0   | Clearly addresses the question.                                  |\n",
        "\n",
        "---\n",
        "\n",
        "## **Overall Assessment**\n",
        "\n",
        "| Metric       | Average Score (3 samples)        | Evaluation                                                |\n",
        "| ------------ | -------------------------------- | --------------------------------------------------------- |\n",
        "| Groundedness | (0.8 + 0.8 + 0.8) / 3 = **0.80** | Decent — answers are mostly grounded in retrieved context |\n",
        "| Faithfulness | (1.0 + 1.0 + 0.6) / 3 = **0.87** | Good — mostly factually correct, minor exaggeration       |\n",
        "| Relevance    | 1.0 across all                   | **Excellent** — answers are always on-topic               |\n",
        "\n",
        "---\n",
        "\n",
        "## **Is This Good or Bad?**\n",
        "\n",
        "### **Good:**\n",
        "\n",
        "* **Relevance is consistently high (1.0)**, which is critical in RAG.\n",
        "* **Faithfulness is strong**, though one answer (Example 3) could be more careful in phrasing.\n",
        "* **Groundedness is decent**, suggesting the model uses retrieved context well but doesn’t quote or align verbatim.\n",
        "\n",
        "### **Could Be Improved:**\n",
        "\n",
        "* Groundedness could improve by more tightly integrating actual phrases or details from the context.\n",
        "* Faithfulness in Example 3 suggests a need for more accurate numeric retention (e.g., \"30,000\" vs. \"thousands\").\n",
        "\n",
        "---\n",
        "\n",
        "## **What We Understand From This**\n",
        "\n",
        "1. The **RAG system is generally performing well**: answers are accurate, relevant, and fairly grounded.\n",
        "2. Most quality issues are around **minor generalizations or partial grounding**, not misinformation.\n",
        "3. The evaluation logic is simple and could be enhanced using:\n",
        "\n",
        "   * **semantic similarity models (e.g., BERTScore)**\n",
        "   * **LLM-based evaluation (e.g., GPT-4 as a judge)**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_OBzpSDCh6bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-U2IZpYChPo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HDiT7PSDiqw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔍 What is RAGAS?\n",
        "\n",
        "https://docs.ragas.io/en/stable/\n",
        "\n",
        "**RAGAS** stands for **Retrieval-Augmented Generation Assessment**.  \n",
        "It’s an **open-source evaluation framework** that helps you **measure the quality** of your RAG pipelines.\n",
        "\n",
        "In simple words:  \n",
        "> “It tells you how good your RAG system is — at retrieving useful chunks, generating accurate and faithful answers, and being self-aware when it doesn’t know.”\n",
        "\n",
        "---\n",
        "\n",
        "## 🧱 Why Do We Need RAGAS?\n",
        "\n",
        "In RAG systems:\n",
        "- The LLM answers a question based on documents retrieved by a search engine.\n",
        "- We want to **ensure each step works well** — both the **retrieval** and the **generation**.\n",
        "\n",
        "🛠 Traditional LLM evaluation (e.g., BLEU, ROUGE) doesn't work well here because:\n",
        "- Answers might be factually correct but worded differently.\n",
        "- We care more about **truthfulness**, **faithfulness**, and **context grounding**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 Key Metrics in RAGAS\n",
        "\n",
        "Let’s walk through **each metric**, explain it in **simple words**, and give a **formula + example**.\n",
        "\n",
        "---\n",
        "\n",
        "### 📘 1. **Faithfulness**\n",
        "\n",
        "**What it asks:**  \n",
        "> \"Is the generated answer truly supported by the retrieved context?\"\n",
        "\n",
        "- It checks if the answer **can be traced back** to the documents retrieved.\n",
        "\n",
        "**Why it matters:**  \n",
        "If the LLM \"hallucinates\" (i.e., makes up stuff), faithfulness will be low.\n",
        "\n",
        "#### ✅ Formula (High-level):\n",
        "Faithfulness = Similarity(generated_answer, grounded_facts_from_context)\n",
        "\n",
        "- Evaluated using **Natural Language Inference (NLI)** models.\n",
        "- Uses labels like “entailment”, “neutral”, “contradiction”\n",
        "\n",
        "#### 🧠 Example:\n",
        "**Context:** \"Employees are eligible for 24 days of leave.\"  \n",
        "**Question:** \"How many leave days do I get?\"  \n",
        "**Answer:** \"You get 30 days of leave.\" ❌  \n",
        "➡️ Not faithful (answer contradicts the context)\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 2. **Context Precision**\n",
        "\n",
        "**What it asks:**  \n",
        "> \"How much of the retrieved context is actually useful for answering the question?\"\n",
        "\n",
        "- It checks how much of the **retrieved info is relevant**.\n",
        "\n",
        "#### ✅ Formula:\n",
        "Context Precision = (# relevant context chunks) / (# total retrieved chunks)\n",
        "\n",
        "#### 🧠 Example:\n",
        "You retrieve 5 chunks, but only 2 were useful →  \n",
        "Context Precision = 2/5 = 0.4 (low)\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 3. **Answer Relevancy**\n",
        "\n",
        "**What it asks:**  \n",
        "> \"How relevant is the generated answer to the actual question?\"\n",
        "\n",
        "- Even if it’s not totally correct, was it **on topic**?\n",
        "\n",
        "#### ✅ Formula:\n",
        "Evaluated using a cross-encoder that measures semantic similarity between **question ↔ answer**\n",
        "\n",
        "#### 🧠 Example:\n",
        "**Question:** \"What’s the deadline to claim reimbursements?\"  \n",
        "**Answer:** \"Employees must serve a 30-day notice period.\"  \n",
        "➡️ Low relevancy (off-topic)\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 4. **Answer Correctness**\n",
        "\n",
        "**What it asks:**  \n",
        "> \"Is the answer factually correct compared to ground truth?\"\n",
        "\n",
        "- Needs labeled test data with correct answers.\n",
        "- Often used during **benchmark testing**.\n",
        "\n",
        "#### ✅ Formula:\n",
        "Correctness = Similarity(predicted_answer, ground_truth_answer)\n",
        "\n",
        "#### 🧠 Example:\n",
        "**Ground Truth:** \"10 business days\"  \n",
        "**Answer:** \"Ten working days\" → ✅ High correctness\n",
        "\n",
        "---\n",
        "\n",
        "### 🚫 5. **Context Recall**\n",
        "\n",
        "**What it asks:**  \n",
        "> \"Did we retrieve all the chunks needed to answer the question completely?\"\n",
        "\n",
        "- Especially useful in **multi-hop QA** (where you need multiple pieces of info).\n",
        "\n",
        "#### ✅ Formula:\n",
        "Context Recall = (# relevant retrieved chunks) / (# relevant total chunks in corpus)\n",
        "\n",
        "This metric is still **experimental** and not always enabled.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔬 Additional Components\n",
        "\n",
        "### ✅ 6. **RAGAS Score** (Overall Composite)\n",
        "\n",
        "This is a **weighted average** of all metrics:  \n",
        "> Faithfulness, Context Precision, Answer Relevancy, Correctness\n",
        "\n",
        "Weights can be configured based on your use case.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Example RAGAS Output\n",
        "\n",
        "| Metric              | Score |\n",
        "|---------------------|-------|\n",
        "| Faithfulness        | 0.88  |\n",
        "| Context Precision   | 0.60  |\n",
        "| Answer Relevancy    | 0.95  |\n",
        "| Answer Correctness  | 0.90  |\n",
        "| **RAGAS Score**     | 0.83  |\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠 How to Use RAGAS in Code\n",
        "\n",
        "```python\n",
        "from ragas.metrics import faithfulness, context_precision, answer_relevancy, answer_correctness\n",
        "from ragas.evaluator import evaluate\n",
        "from ragas.dataset import Dataset\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "# Load your question-answer-context triples\n",
        "hf_dataset = HFDataset.from_dict({\n",
        "    \"question\": [...],\n",
        "    \"contexts\": [...],\n",
        "    \"answer\": [...],\n",
        "    \"ground_truth\": [...],  # optional for correctness\n",
        "})\n",
        "\n",
        "# Convert to RAGAS Dataset\n",
        "ragas_dataset = Dataset(hf_dataset)\n",
        "\n",
        "# Evaluate\n",
        "results = evaluate(\n",
        "    ragas_dataset,\n",
        "    metrics=[faithfulness, context_precision, answer_relevancy, answer_correctness]\n",
        ")\n",
        "print(results)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 When Should You Use RAGAS?\n",
        "\n",
        "- ✅ Benchmarking different RAG pipelines\n",
        "- ✅ Comparing vector DBs (e.g., FAISS vs Chroma)\n",
        "- ✅ Checking model hallucination\n",
        "- ✅ Monitoring production quality of GenAI apps\n",
        "- ✅ Deciding fine-tuning vs retrieval\n",
        "\n",
        "---\n",
        "\n",
        "## 📦 Installation\n",
        "\n",
        "```bash\n",
        "pip install ragas\n",
        "```\n",
        "\n",
        "Also requires:\n",
        "```bash\n",
        "pip install datasets transformers evaluate\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Summary Table\n",
        "\n",
        "| Metric            | Measures                         | Needs Ground Truth? | Model Used           |\n",
        "|------------------|----------------------------------|----------------------|----------------------|\n",
        "| Faithfulness      | Is answer supported by context? | ❌                   | NLI model            |\n",
        "| Context Precision | How much context is useful?     | ❌                   | Semantic similarity  |\n",
        "| Answer Relevancy  | Is answer on-topic?             | ❌                   | Cross-encoder        |\n",
        "| Answer Correctness| Is answer factually right?      | ✅                   | Text similarity      |\n",
        "| Context Recall    | All info retrieved?             | Optional             | Semantic check       |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4BpGXOD3jWCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu ragas -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btJUamlHiroP",
        "outputId": "cabcd5bc-9138-4992-bf02-969050cd022e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/190.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m184.3/190.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import BleuScore\n",
        "\n",
        "test_data = {\n",
        "    \"user_input\": \"summarise given text\\nThe company reported an 8% rise in Q3 2024, driven by strong performance in the Asian market. Sales in this region have significantly contributed to the overall growth. Analysts attribute this success to strategic marketing and product localization. The positive trend in the Asian market is expected to continue into the next quarter.\",\n",
        "    \"response\": \"The company experienced an 8% increase in Q3 2024, largely due to effective marketing strategies and product adaptation, with expectations of continued growth in the coming quarter.\",\n",
        "    \"reference\": \"The company reported an 8% growth in Q3 2024, primarily driven by strong sales in the Asian market, attributed to strategic marketing and localized products, with continued growth anticipated in the next quarter.\"\n",
        "}\n",
        "metric = BleuScore()\n",
        "# BLEU (Bilingual Evaluation Understudy) measures how close a generated response is to a human-written reference.\n",
        "# Higher BLEU = better quality generation\n",
        "test_data = SingleTurnSample(**test_data)\n",
        "metric.single_turn_score(test_data)\n",
        "\n",
        "# It compares the \"response\" with the \"reference\" and returns a score between 0 and 1.\n",
        "# A score closer to 1 means the generated response is very close to the ideal answer.\n",
        "# Example output: 0.1371 means low similarity — possibly due to word choice differences or structure mismatch."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsGsrWwUir9f",
        "outputId": "d7902953-0160-40a4-8e71-e3e92d9c921f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13718598426177148"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics import BleuScore\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    response=\"The Eiffel Tower is located in India.\",\n",
        "    reference=\"The Eiffel Tower is located in Paris.\"\n",
        ")\n",
        "\n",
        "scorer = BleuScore()\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KdN2TXOitbD",
        "outputId": "4595d893-b7d3-4f13-b783-ec5c0f235f4d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7071067811865478"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_QGAZrhvi-K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔴 What is ROUGE?\n",
        "\n",
        "**ROUGE** stands for:  \n",
        "> **Recall-Oriented Understudy for Gisting Evaluation**\n",
        "\n",
        "It’s a set of **metrics used to evaluate automatic summarization or generation tasks**, like:\n",
        "\n",
        "- Summarization\n",
        "- Question Answering\n",
        "- Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Simple Explanation\n",
        "\n",
        "> ROUGE measures **how much of the reference (ground truth) answer appears in the generated answer**.\n",
        "\n",
        "While **BLEU** focuses on **precision** (how much of the generated content is correct),  \n",
        "**ROUGE focuses more on recall** (how much of the reference was captured).\n",
        "\n",
        "---\n",
        "\n",
        "## 🔢 ROUGE Variants (Most Used)\n",
        "\n",
        "| Type        | Meaning                                                       |\n",
        "|-------------|---------------------------------------------------------------|\n",
        "| **ROUGE-1** | Overlap of **unigrams** (single words)                        |\n",
        "| **ROUGE-2** | Overlap of **bigrams** (pairs of words)                       |\n",
        "| **ROUGE-L** | Longest Common Subsequence (LCS) between reference & result   |\n",
        "\n",
        "---\n",
        "\n",
        "## 📐 How is ROUGE Calculated?\n",
        "\n",
        "Let’s use **ROUGE-1** as an example.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 1. **Unigrams (ROUGE-1)**\n",
        "\n",
        "Suppose:\n",
        "\n",
        "- **Generated Answer:** \"Employees get 24 leave days\"\n",
        "- **Reference Answer:** \"Employees get 24 paid leave days\"\n",
        "\n",
        "Unigrams in Generated = {employees, get, 24, leave, days}  \n",
        "Unigrams in Reference = {employees, get, 24, paid, leave, days}\n",
        "\n",
        "**Overlap (common):** employees, get, 24, leave, days → 5 words\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 2. **Formula: ROUGE Recall, Precision, F1**\n",
        "\n",
        "Let’s define:\n",
        "\n",
        "- **A** = words in generated answer  \n",
        "- **B** = words in reference answer  \n",
        "- **Overlap** = common words in both\n",
        "\n",
        "#### 📘 ROUGE Recall:\n",
        "\\[\n",
        "\\text{Recall} = \\frac{\\text{# overlapping words}}{\\text{# words in reference}}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "= \\frac{5}{6} = 0.83\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "#### 📙 ROUGE Precision:\n",
        "\\[\n",
        "\\text{Precision} = \\frac{\\text{# overlapping words}}{\\text{# words in generated}}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "= \\frac{5}{5} = 1.0\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "#### 📗 ROUGE-F1:\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Example Summary Table\n",
        "\n",
        "| Metric      | Score |\n",
        "|-------------|-------|\n",
        "| ROUGE-1     | 0.91  |\n",
        "| ROUGE-2     | 0.85  |\n",
        "| ROUGE-L     | 0.92  |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔬 ROUGE vs BLEU\n",
        "\n",
        "| Feature              | BLEU                         | ROUGE                        |\n",
        "|----------------------|------------------------------|------------------------------|\n",
        "| Focus                | Precision                    | Recall                       |\n",
        "| Goal                 | How much of gen is correct   | How much of ref is covered   |\n",
        "| Good For             | Translation                  | Summarization, QA            |\n",
        "| Common in            | Machine Translation          | Summarization & RAG          |\n",
        "| Metric               | n-gram overlap (gen → ref)   | n-gram overlap (ref → gen)   |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 ROUGE in RAGAS\n",
        "\n",
        "ROUGE is **not officially part of core RAGAS metrics** (which uses faithfulness, answer relevancy, etc.), but:\n",
        "\n",
        "- You **can compute ROUGE** to compare **model-generated answers vs. ground truth**\n",
        "- It helps when evaluating **extractive summarization** or **QA answer overlap**\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠 How to Compute ROUGE in Python\n",
        "\n",
        "```python\n",
        "from evaluate import load\n",
        "\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "# Sample prediction and reference\n",
        "predictions = [\"Employees get 24 leave days.\"]\n",
        "references = [\"Employees get 24 paid leave days.\"]\n",
        "\n",
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(results)\n",
        "```\n",
        "\n",
        "### Output:\n",
        "```python\n",
        "{\n",
        "  'rouge1': 0.83,\n",
        "  'rouge2': 0.75,\n",
        "  'rougeL': 0.85,\n",
        "  'rougeLsum': 0.85\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Real-World RAG Example\n",
        "\n",
        "**Question:** \"How many paid leaves do I get?\"\n",
        "\n",
        "- **Reference:** \"Employees get 24 paid leave days.\"\n",
        "- **Model A Answer:** \"Employees get 24 leave days.\"\n",
        "- **Model B Answer:** \"Employees receive 30 days off.\"\n",
        "\n",
        "| Metric     | Model A     | Model B     |\n",
        "|------------|-------------|-------------|\n",
        "| ROUGE-1    | 0.91        | 0.67        |\n",
        "| ROUGE-2    | 0.82        | 0.44        |\n",
        "| ROUGE-L    | 0.92        | 0.56        |\n",
        "\n",
        "🧠 **Model A** is closer in language and content. ROUGE confirms that.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary\n",
        "\n",
        "| Term        | Meaning |\n",
        "|-------------|---------|\n",
        "| ROUGE-1     | Recall-based overlap of unigrams (words)  \n",
        "| ROUGE-2     | Bigram overlap (word pairs)  \n",
        "| ROUGE-L     | Longest common sequence between reference and generated  \n",
        "| ROUGE Recall| % of reference captured  \n",
        "| ROUGE F1    | Balance of precision and recall\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Use ROUGE When:\n",
        "- Evaluating **QA or summarization** output quality\n",
        "- Comparing **generated answers to ground-truth**\n",
        "- Complementing semantic metrics (like **faithfulness** in RAGAS)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Wch2dIFOlTfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yop4kmQgjBIt",
        "outputId": "92f223ca-7c8a-4492-eb9f-6a11553a87e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics import RougeScore\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    response=\"The Eiffel Tower is located in India.\",\n",
        "    reference=\"The Eiffel Tower is located in Paris.\"\n",
        ")\n",
        "\n",
        "scorer = RougeScore()\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk28Tkz5jBF4",
        "outputId": "aa97468c-4b74-44ba-d6bd-61b5bad707e1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8571428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics import RougeScore\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    response=\"The Eiffel Tower is located in India.\",\n",
        "    reference=\"The Eiffel Tower is located in Paris.\"\n",
        ")\n",
        "\n",
        "scorer = RougeScore(rouge_type=\"rouge1\")\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pqeBqtYjBke",
        "outputId": "6318fa99-86c9-4a8e-e673-e7cea5ac2dc1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8571428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics import RougeScore\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    response=\"The Eiffel Tower is located in India.\",\n",
        "    reference=\"The Eiffel Tower is located in Paris.\"\n",
        ")\n",
        "\n",
        "scorer = RougeScore(mode=\"recall\")\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFBc8b3OjGt2",
        "outputId": "92ef9b4c-2f52-49ca-8255-def224954705"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8571428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics import ExactMatch\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    response=\"India\",\n",
        "    reference=\"Paris\"\n",
        ")\n",
        "\n",
        "scorer = ExactMatch()\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EHZ3QxujKzw",
        "outputId": "4fffac3e-3664-4b06-a1a4-da41db0e9a84"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics import StringPresence\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    response=\"The Eiffel Tower is located in India.\",\n",
        "    reference=\"Eiffel Tower\"\n",
        ")\n",
        "scorer = StringPresence()\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m70P_I4ujMiZ",
        "outputId": "5d54af8a-1b1d-4fe0-e536-c6c6dc72b470"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Happy Learning"
      ],
      "metadata": {
        "id": "50rRp6YQmIiv"
      }
    }
  ]
}