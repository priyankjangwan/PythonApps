{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem Statement**\n",
        "\n",
        "In real-world applications, organizations often store important information in documents such as **policy manuals, handbooks, or technical PDFs**. Traditional language models like GPT-4 do not have access to these documents unless explicitly given the data.\n",
        "\n",
        "This project demonstrates how to build a **Retrieval-Augmented Generation (RAG)** system using LangChain and OpenAI. It allows the language model to **search inside a PDF document**, retrieve relevant information, and generate accurate, context-specific answers.\n",
        "\n",
        "---\n",
        "\n",
        "## **Objectives**\n",
        "\n",
        "1. Load content from a PDF file into a LangChain-compatible format.\n",
        "2. Split the PDF content into manageable chunks for efficient vector storage.\n",
        "3. Generate embeddings for each chunk using OpenAI Embeddings.\n",
        "4. Store and search the chunks using the FAISS vector store.\n",
        "5. Use GPT-4 Turbo as the language model to generate context-aware answers using retrieved chunks.\n",
        "6. Ensure that the implementation uses the latest LangChain and OpenAI APIs without any deprecation warnings.\n",
        "\n",
        "---\n",
        "\n",
        "## **Expected Outcomes**\n",
        "\n",
        "- A working RAG pipeline that uses PDF content as a knowledge base.\n",
        "- The system can answer user queries based on the information stored inside the PDF.\n",
        "- It will retrieve relevant passages from the PDF and use them to generate answers via GPT-4.\n",
        "- The setup will be modular, extensible, and aligned with the latest LangChain version.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sZ2mM6URVKJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ipso9DqRQ1WK"
      },
      "outputs": [],
      "source": [
        "# RAG Architecture\n",
        "# https://miro.medium.com/v2/resize:fit:1400/1*YLrQl5CM7NjQPcfTCrf-sQ.png\n",
        "\n",
        "# https://miro.medium.com/v2/resize:fit:1400/1*yfeUrFCr9oEVZofS8TvDEg.png"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzW3nfMaXA1t",
        "outputId": "7bf2232f-9378-4bce-b5bf-841324aab892"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai -q"
      ],
      "metadata": {
        "id": "rWqBLfuJXDlG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader # a class used to load text from PDF files\n",
        "\n",
        "# Imports a powerful text-splitting tool that breaks large documents into manageable chunks for embedding.\n",
        "# LLMs have context length limits, so splitting documents into chunks is essential.\n",
        "# Why recursive?\n",
        "# It tries to split at logical boundaries (like paragraphs, sentences, etc.) before falling back to raw character limits.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# OpenAIEmbeddings - to convert text into vector format using OpenAI’s embedding API\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "\n",
        "from langchain.vectorstores import FAISS # a fast in-memory vector search engine used for semantic search.\n",
        "\n",
        "\"\"\"Imports RetrievalQA, a chain that connects:\n",
        "-A retriever (like FAISS)\n",
        "-A language model (like GPT): To answer questions using retrieved context.\n",
        "\n",
        "This is the heart of a RAG system — combining search + generation\"\"\"\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "a07nzYEtWjbX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q\n",
        "import openai\n"
      ],
      "metadata": {
        "id": "o6r-Ot1nXdPn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Way2 (works only in Colab NB)\n",
        "\n",
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Update the API key by updating the environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "UhLRJTO0W9kq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A3DQxkVYIHA",
        "outputId": "d517b52d-6efd-4d59-d07d-d3b31ff8ba02"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/313.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m307.2/313.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/docs/integrations/document_loaders/\n",
        "# Step 2: Load PDF document (Replace with your actual file path)\n",
        "pdf_path = \"docs/company_policy.pdf\"  # Make sure this file exists\n",
        "\n",
        "# PyPDFLoader - Extracts text content from each page of the PDF. Treats each page as a separate document.\n",
        "# Ignores images or tables (only extracts plain text). If your PDF has scanned images or rich tables, they won’t be properly extracted\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "# Print the number of pages loaded\n",
        "print(f\"Loaded {len(documents)} pages from the PDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rNUZiHHXbDb",
        "outputId": "672166aa-6aa0-43de-c3d2-c07908bd8710"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 24 pages from the PDF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# documents"
      ],
      "metadata": {
        "id": "V_TWcfV9X-jV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. PDF Loaders\n",
        "Loader =>\tUse When...\t=> Handles Images/Tables?\t=> Code Example\n",
        "PyPDFLoader\tSimple text-based PDFs    => No\t=> loader = PyPDFLoader(\"path/to/pdf\")\n",
        "PDFMinerLoader\tNeeds layout-aware text extraction => No\t=>\tPDFMinerLoader(\"file.pdf\")\n",
        "PDFPlumberLoader\t=> Needs table extraction => Basic tables\t=> PDFPlumberLoader(\"file.pdf\")\n",
        "UnstructuredPDFLoader\t=> Complex structure, mixed text, tables, images => Yes (OCR + layout)    => UnstructuredPDFLoader(\"file.pdf\")\n",
        "PyMuPDFLoader\tText + metadata (fast, accurate) => (Limited)\tPyMuPDFLoader(\"file.pdf\")\n",
        "\n",
        "2. Word Documents (DOC / DOCX)\n",
        "Loader\t=> Use When...\t=> Code\n",
        "UnstructuredWordDocumentLoader\tYou have .docx files\tUnstructuredWordDocumentLoader(\"file.docx\")\n",
        "Docx2txtLoader\tBasic text extraction from .docx\tDocx2txtLoader(\"file.docx\")\n",
        "\n",
        "3. Excel Files (XLS / XLSX)\n",
        "Loader\tUse When...\tCode\n",
        "UnstructuredExcelLoader\tFull sheet extraction\tUnstructuredExcelLoader(\"file.xlsx\")\n",
        "PandasExcelLoader\tStructured loading using pandas\tPandasExcelLoader(\"file.xlsx\")\n",
        "\n",
        "4. CSV Files\n",
        "Loader\tUse When...\tCode\n",
        "CSVLoader\tSimple CSV reading\tCSVLoader(\"file.csv\")\n",
        "PandasCSVLoader\tUse Pandas for control & filtering\tPandasCSVLoader(\"file.csv\")\n",
        "\n",
        "5. JSON / JSONL\n",
        "Loader\tUse When...\tCode\n",
        "JSONLoader\tSimple .json file with nested fields    JSONLoader(file_path=\"file.json\", jq_schema='.data[].text', text_content=False)\n",
        "\n",
        "6. Web & HTML\n",
        "Loader\tUse When...\tCode\n",
        "WebBaseLoader\tLoad content from a URL\tWebBaseLoader(\"https://example.com\")\n",
        "UnstructuredHTMLLoader\tClean up raw HTML structure\tUnstructuredHTMLLoader(\"file.html\")\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Load PDF with Tables using PDFPlumberLoader\n",
        "from langchain.document_loaders import PDFPlumberLoader\n",
        "loader = PDFPlumberLoader(\"report_with_tables.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "Load Excel Sheet using UnstructuredExcelLoader\n",
        "from langchain.document_loaders import UnstructuredExcelLoader\n",
        "loader = UnstructuredExcelLoader(\"financials.xlsx\")\n",
        "documents = loader.load()\n",
        "\n",
        "Load JSON File\n",
        "from langchain.document_loaders import JSONLoader\n",
        "loader = JSONLoader(file_path=\"data.json\", jq_schema=\".records[].summary\", text_content=True)\n",
        "documents = loader.load()\n",
        "\n",
        "Load DOCX File\n",
        "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
        "loader = UnstructuredWordDocumentLoader(\"policy.docx\")\n",
        "documents = loader.load()\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "How to Choose the Right Loader?\n",
        "| Content Type               | Recommended Loader                                          | Why                           |\n",
        "| -------------------------- | ----------------------------------------------------------- | ----------------------------- |\n",
        "| Simple PDFs                | `PyPDFLoader`                                               | Fast, reliable                |\n",
        "| PDFs with tables           | `PDFPlumberLoader`                                          | Can extract table content     |\n",
        "| PDFs with images / scanned | `UnstructuredPDFLoader`                                     | Uses OCR and layout modeling  |\n",
        "| Word / Excel               | `UnstructuredWordDocumentLoader`, `UnstructuredExcelLoader` | Best structure preservation   |\n",
        "| CSV / JSON                 | `PandasCSVLoader`, `JSONLoader`                             | Allows flexible preprocessing |\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0JBgrMRKYWzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split PDF text into smaller chunks\n",
        "# https://miro.medium.com/v2/resize:fit:1400/1*yfeUrFCr9oEVZofS8TvDEg.png\n",
        "# RecursiveCharacterTextSplitter tries to split at logical boundaries (paragraph → sentence → word → character) — hence, recursive\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500, # chunk_size=500: Each chunk will be up to 500 characters long.\n",
        "    chunk_overlap=100 # chunk_overlap=100: The last 100 characters of one chunk are repeated in the next chunk for context continuity.\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Print the number of chunks created\n",
        "print(f\"Created {len(chunks)} text chunks from the PDF.\")\n",
        "\n",
        "# Print the second chunk as an example\n",
        "print(f\"Second chunk: {chunks[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR85wqItZBY8",
        "outputId": "0f8aa062-1178-4f04-8932-da6dcee175c9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 122 text chunks from the PDF.\n",
            "Second chunk: page_content='SPIL Corporate HR Policies  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Section 1: Introduction  \n",
            " \n",
            "This handbook is the summary of the policies, procedures, guidance and benefits to the employees \n",
            "and organization. It is an introduction to our vision, mission, values, what you expect from us and \n",
            "what we expect from you. We believe that employees are the assets of the organization and to \n",
            "understand them the positive work environment play an important role.' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2020-08-26T06:56:00+00:00', 'author': 'hr', 'moddate': '2020-08-26T06:56:00+00:00', 'source': 'docs/company_policy.pdf', 'total_pages': 24, 'page': 1, 'page_label': '2'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the last chunk as an example\n",
        "print(f\"Last chunk: {chunks[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVEKpzBGZOgn",
        "outputId": "5d33356d-0fb1-493c-c8c4-554b3c52a812"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last chunk: page_content='Section : 20  Review and Amendment  \n",
            "Management shall review this policy periodically and amendments required, if any shall be made \n",
            "accordingly.  \n",
            "Section : 21 Residual Power \n",
            "This policy is basically guidelines and the management reserves the right to withdraw / modify to \n",
            "suit organization’s philosophy at any time without assigning any reason whatsoever. \n",
            "EFFECTIVE \n",
            "Commencement Of Policy  August 21, 2018  \n",
            " \n",
            " \n",
            " \n",
            "Approved By : ___________SD/-_______________ \n",
            "Mr Sanjay Agarwal - CMD' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2020-08-26T06:56:00+00:00', 'author': 'hr', 'moddate': '2020-08-26T06:56:00+00:00', 'source': 'docs/company_policy.pdf', 'total_pages': 24, 'page': 23, 'page_label': '24'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Why Is Chunking So Important in RAG?\n",
        "LLMs (like GPT-4) have a limited context window (e.g., 8K or 32K tokens), so you can't send the whole document.\n",
        "Chunking allows:\n",
        "-Semantic search over smaller pieces of the document\n",
        "-Better matching with the user's query\n",
        "-Faster retrieval, better accuracy\n",
        "\n",
        "Common Chunking Strategies (Used in Industry)\n",
        "| Splitter Class                          | Description                                           | Use Case                                           |\n",
        "| --------------------------------------- | ----------------------------------------------------- | -------------------------------------------------- |\n",
        "| `RecursiveCharacterTextSplitter`        | Splits at paragraphs → sentences → words → characters | Best for generic documents (PDFs, policies, books) |\n",
        "| `CharacterTextSplitter`                 | Splits at fixed character limits (naive)              | Simple logs, structured text                       |\n",
        "| `TokenTextSplitter`                     | Splits by token count (uses tokenizer like tiktoken)  | Precise control for GPT-3.5/4                      |\n",
        "| `SentenceTransformersTokenTextSplitter` | Aware of sentence boundaries + tokens                 | Best for multilingual and NLP-heavy documents      |\n",
        "| `MarkdownHeaderTextSplitter`            | Splits based on Markdown headers                      | Technical docs, blog posts, notebooks              |\n",
        "| `HTMLHeaderTextSplitter`                | Splits based on HTML tags                             | Websites, web-scraped data                         |\n",
        "| `Language` Splitters                    | Code-aware (Python, JS, etc.)                         | Splits by function/class — great for code docs     |\n",
        "\n",
        "\n",
        "How to Choose chunk_size and chunk_overlap?\n",
        "chunk_size: How big each chunk is (in characters or tokens)\n",
        "| Scenario                       | Recommended Chunk Size               |\n",
        "| ------------------------------ | ------------------------------------ |\n",
        "| Short emails, chat transcripts | 300–500 chars                        |\n",
        "| Policies, contracts, PDFs      | 500–1000 chars                       |\n",
        "| Technical manuals or FAQs      | 800–1500 chars                       |\n",
        "| Code or JSON files             | 100–300 chars (or by function/class) |\n",
        "| Scientific papers (dense info) | 1000–1500 chars or 256–512 tokens    |\n",
        "| For OpenAI GPT-4 (8k model)    | Chunk size ≤ 1000 tokens             |\n",
        "| For GPT-4 Turbo (128k model)   | Chunk size ≤ 3000 tokens             |\n",
        "\n",
        "chunk_overlap: How much to repeat from previous chunk\n",
        "| Purpose                              | Suggested Overlap                                      |\n",
        "| ------------------------------------ | ------------------------------------------------------ |\n",
        "| Maintain sentence continuity         | 10–20% of chunk\\_size (e.g., 100 chars for 500 chunks) |\n",
        "| Prevent cutoff of entity names       | 100–150 chars                                          |\n",
        "| Use with semantic search (retriever) | 10–20%                                                 |\n",
        "| Use with QA or summarization         | 100–200 chars                                          |\n",
        "\n",
        "Example Best Practices:\n",
        "Contract Documents:\n",
        "RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "Contracts often have long clauses — more overlap helps preserve clause boundaries.\n",
        "\n",
        "FAQs or Web Pages:\n",
        "RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)\n",
        "\n",
        "Code Files:\n",
        "PythonCodeTextSplitter(chunk_lines=30, chunk_overlap=10)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cE6a_5RdZYev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create embeddings for each chunk using OpenAI\n",
        "embeddings = OpenAIEmbeddings(api_key=openai.api_key) # text-embedding-ada-002\n",
        "# embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "7H6kYoeCZbiD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbsuLPY7bGmN",
        "outputId": "e78c54f9-ad6e-4587-cc32-ee3db4337e96"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Store chunks in FAISS vector store\n",
        "\"\"\"Converts a list of text chunks into vectors using the embeddings model (e.g., OpenAIEmbeddings).\n",
        "Stores those vectors inside a FAISS vector store\n",
        "Returns a vectorstore object that supports fast semantic search (using cosine or inner product similarity)\"\"\"\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "yKwy4Rvza8sx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.linkedin.com/posts/drdarshaningle-instructor_best-tips-for-vectordb-selection-for-genai-activity-7174250159881547776-wL3U?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAicoJwBJOlKakGIOE2ywfb2Viu908DpjoI\n",
        "\n",
        "\"\"\"\n",
        "What is a Vector Store?\n",
        "A vector store (or vector database) is a specialized storage engine that:\n",
        "-Stores high-dimensional embeddings (vectors)\n",
        "-Supports similarity search (e.g., cosine similarity, dot product)\n",
        "-Returns the most relevant stored chunks given a query\n",
        "\n",
        "Used in:\n",
        "-Semantic search\n",
        "-RAG systems (retrieval-augmented generation)\n",
        "-Recommendation engines\n",
        "-Image/audio search\n",
        "\n",
        "Common Vector Stores in the Industry (Open-source + Paid)\n",
        "| Name                     | Type                     | Open Source | Cloud Hosted           | Common Use Case                       | Notes                                   |\n",
        "| ------------------------ | ------------------------ | ----------- | ---------------------- | ------------------------------------- | --------------------------------------- |\n",
        "| **FAISS**                | In-memory                | Yes         | No                     | Fast local prototyping                | Lightweight, fast, no persistence       |\n",
        "| **Chroma**               | Embedded / local         | Yes         | Yes (through services) | Quick, simple RAG systems             | Comes with LangChain by default         |\n",
        "| **Weaviate**             | Cloud-native             | Yes         | Yes                    | Enterprise-scale semantic search      | Supports hybrid search (text + vector)  |\n",
        "| **Pinecone**             | Cloud-hosted             | No          | Yes                    | Scalable production RAG               | High availability, fast search          |\n",
        "| **Qdrant**               | Cloud-native             | Yes         | Yes                    | High-perf vector search with metadata | Good for hybrid + multi-modal           |\n",
        "| **Milvus**               | Cloud/on-prem            | Yes         | Yes                    | AI at scale (video, image, text)      | GPU acceleration available              |\n",
        "| **ElasticSearch + k-NN** | General-purpose + plugin | Partially   | Yes                    | Enterprises with existing ES stack    | Not optimized for dense vectors         |\n",
        "| **Redis + Redis Vector** | General-purpose + plugin | Yes         | Yes                    | Realtime vector search                | Limited advanced vector search features |\n",
        "\n",
        "Vector Store Comparisons: Speed, Cost, and Use Cases\n",
        "1. FAISS\n",
        "Speed: Extremely fast (C++ backend, in-memory)\n",
        "Cost: Free (open source)\n",
        "Storage: In-memory only (unless manually persisted)\n",
        "Best for: Prototyping, small datasets, no need for persistence\n",
        "Limitations: Not suitable for production unless wrapped in a persistence layer\n",
        "\n",
        "2. Pinecone\n",
        "Speed: High (vector indexes stored in cloud)\n",
        "Cost: Paid, with free tier (based on vector count + queries)\n",
        "Best for: Scalable production RAG with real-time search\n",
        "Organizations: Used by startups to large SaaS platforms\n",
        "Strengths: Metadata filtering, hybrid search, horizontal scaling\n",
        "\n",
        "3. Qdrant\n",
        "Speed: Very good (Rust backend)\n",
        "Cost: Free self-hosted; cloud pricing available\n",
        "Best for: Production systems where filtering and custom payloads are needed\n",
        "Organizations: ML teams, research labs, open-source apps\n",
        "Strengths: JSON metadata, filtering, search inside images, multi-modal\n",
        "\n",
        "4. Weaviate\n",
        "Speed: Very good\n",
        "Cost: Free (local) + paid cloud tier\n",
        "Best for: NLP apps, semantic search, hybrid search\n",
        "Strengths: GraphQL API, hybrid (BM25 + dense), built-in classification\n",
        "\n",
        "5. Chroma\n",
        "Speed: Fast for small-to-medium workloads\n",
        "Cost: Free (open source)\n",
        "Best for: Educational use, small RAGs\n",
        "Limitations: Not built for production-scale retrieval yet\n",
        "\n",
        "6. Milvus\n",
        "Speed: Excellent (supports billions of vectors)\n",
        "Cost: Free (community edition); paid cloud (Zilliz)\n",
        "Best for: Large-scale systems (multi-modal), image/video/audio search\n",
        "Organizations: Fintech, bioinformatics, autonomous systems\n",
        "\n",
        "7. Redis Vector\n",
        "Speed: Real-time optimized\n",
        "Cost: Free open source; paid Redis Enterprise\n",
        "Best for: If Redis is already part of your infra (e.g., caching, real-time apps)\n",
        "Limitations: Lacks advanced vector indexing options\n",
        "\n",
        "Which Vector Store Should You Use?\n",
        "Use FAISS when:\n",
        "You’re prototyping or building a small RAG system\n",
        "You want speed without infrastructure\n",
        "You don’t need persistence across sessions\n",
        "\n",
        "Use Pinecone when:\n",
        "You need scalable, production-grade retrieval\n",
        "You want a managed solution (no infra worries)\n",
        "You’re working with large document sets\n",
        "\n",
        "Use Qdrant or Weaviate when:\n",
        "You want metadata filtering\n",
        "You want self-hosted + production-grade performance\n",
        "You want to store documents + embeddings + metadata in one place\n",
        "\n",
        "Use Milvus when:\n",
        "You're working on large, multi-modal data\n",
        "You need high throughput and GPU support\n",
        "\n",
        "Real-World Scenarios:\n",
        "| Use Case                                | Vector Store        |\n",
        "| --------------------------------------- | ------------------- |\n",
        "| Internal policy RAG for small org       | FAISS / Chroma      |\n",
        "| Customer support FAQ bot (medium scale) | Qdrant / Weaviate   |\n",
        "| Public-facing RAG search engine         | Pinecone / Weaviate |\n",
        "| GenAI for PDFs + image documents        | Qdrant / Milvus     |\n",
        "| Prototyping with LangChain locally      | FAISS / Chroma      |\n",
        "\n",
        "Summary Table:\n",
        "| Feature              | FAISS | Pinecone | Qdrant | Weaviate | Chroma  | Milvus |\n",
        "| -------------------- | ----- | -------- | ------ | -------- | ------- | ------ |\n",
        "| Open Source          | Yes   | No       | Yes    | Yes      | Yes     | Yes    |\n",
        "| Cloud Hosted Option  | No    | Yes      | Yes    | Yes      | Limited | Yes    |\n",
        "| Metadata Filtering   | No    | Yes      | Yes    | Yes      | Limited | Yes    |\n",
        "| Multi-modal Support  | No    | No       | Yes    | Limited  | No      | Yes    |\n",
        "| Persistence Built-in | No    | Yes      | Yes    | Yes      | Yes     | Yes    |\n",
        "| Production Ready     | No    | Yes      | Yes    | Yes      | No      | Yes    |\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PT36GPE8bDhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Initialize GPT-4 Turbo via LangChain\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4-turbo\",\n",
        "    temperature=0.3,\n",
        "    api_key=openai.api_key\n",
        ")"
      ],
      "metadata": {
        "id": "jd9643v2dIUq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Create a RetrievalQA chain (retriever + LLM)\n",
        "\n",
        "\"\"\"Converts your vectorstore (FAISS in this case) into a retriever object.\n",
        "A retriever knows how to fetch relevant chunks from the vector store using a query.\n",
        "search_kwargs={\"k\": 3} - return the top 3 most similar chunks (based on vector similarity) for any question.\"\"\"\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True # Also returns the original source chunks used in generating the answer\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "What Happens Behind the Scenes in RetrievalQA:\n",
        "-User provides a question → rag_chain.invoke({\"query\": \"...\"})\n",
        "-retriever fetches top k matching chunks from FAISS\n",
        "-LangChain injects those chunks into the prompt\n",
        "-llm (e.g., GPT-4) uses that context to generate a grounded answer\n",
        "-Optionally, return_source_documents=True lets you see the supporting evidence\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "_N3PkRcadTCF",
        "outputId": "65abd32d-3a98-4a98-ac2d-b89b173864b8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWhat Happens Behind the Scenes in RetrievalQA:\\n-User provides a question → rag_chain.invoke({\"query\": \"...\"})\\n-retriever fetches top k matching chunks from FAISS\\n-LangChain injects those chunks into the prompt\\n-llm (e.g., GPT-4) uses that context to generate a grounded answer\\n-Optionally, return_source_documents=True lets you see the supporting evidence\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Function to ask questions based on the PDF content\n",
        "def ask_pdf_agent(query: str):\n",
        "    print(f\"\\nUser Query: {query}\")\n",
        "    result = rag_chain.invoke({\"query\": query})\n",
        "    # print(\"==\"*30)\n",
        "    # print(f\"Entire Result: {result}\")\n",
        "    # print(\"==\"*30)\n",
        "    print()\n",
        "    print(\"\\nAnswer:\")\n",
        "    print(result[\"result\"])\n",
        "    print(\"\\nRetrieved Passages:\")\n",
        "    for doc in result[\"source_documents\"]:\n",
        "        print(\"-->\", doc.page_content.strip()[:200], \"...\")"
      ],
      "metadata": {
        "id": "WGUMWRo7dVwS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Run a sample query\n",
        "# ask_pdf_agent(\"What is the company's remote work policy?\")\n",
        "ask_pdf_agent(\"What does SPIL stand for?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdsYZucldjfH",
        "outputId": "97878137-9432-4da6-b03d-bf6260596136"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query: What does SPIL stand for?\n",
            "\n",
            "\n",
            "Answer:\n",
            "SPIL stands for Sirca Paints India Limited.\n",
            "\n",
            "Retrieved Passages:\n",
            "--> Section 3: Recruitment  \n",
            " \n",
            "The company policy on recruitment strives for equal opportunity to all irrespective of any \n",
            "distinction of gender, sexual orientation, caste or any disable applicants. All a ...\n",
            "--> SPIL Corporate HR Policies  \n",
            " \n",
            " \n",
            " \n",
            "Section 2: Company Profile \n",
            " \n",
            "SPIL is a company engaged in marketing and trading/distribution of wood coatings and allied \n",
            "products. It is the first company to launc ...\n",
            "--> Resources Department of SPIL.  \n",
            " \n",
            "Applicability  \n",
            " \n",
            "This EHB will be applicable to the employees working in Sirca Paints Ind ia Limited (SPIL) w.e.f \n",
            "August 21, 2020 . This book contains all the notic ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_pdf_agent(\"How many leaves per month is an employee eligible for?\")\n",
        "\n",
        "# Q2:\n",
        "# How many leaves per month is an employee eligible for?\n",
        "\n",
        "# A2:\n",
        "# An employee is eligible for 2 leaves per month (pro-rated based on joining date).\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpkUo9r_dk94",
        "outputId": "1c3fea1a-3ca2-4690-d815-4dfc301cec9a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query: How many leaves per month is an employee eligible for?\n",
            "\n",
            "\n",
            "Answer:\n",
            "An employee is eligible for 2 leaves per month.\n",
            "\n",
            "Retrieved Passages:\n",
            "--> g) Employees who are on “Official Duty” (OD) or “On Tour” (OT) are requested to take the \n",
            "written approval in advance from their Reporting Officer and submit the same to the HR \n",
            "Department for record  ...\n",
            "--> i) The Leave given to the employee will be counted on Financial Year (i.e. April to March) and the \n",
            "balance as on March will be credited to the next financial year of employee leave balance \n",
            "account.  ...\n",
            "--> completion of month. The leave given to the employee will be strictly based on the Date of \n",
            "Joining (i.e. Pro-Rata Basis). Employee who joins on the following dates will be eligible for the \n",
            "leave.  \n",
            " ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_pdf_agent(\"What does the acronym “RESPECT” in SPIL’s sales vision stand for?\")\n",
        "\n",
        "# Q3:\n",
        "# What does the acronym “RESPECT” in SPIL’s sales vision stand for?\n",
        "\n",
        "# A3:\n",
        "\n",
        "# R: Reliability\n",
        "\n",
        "# E: Excellence\n",
        "\n",
        "# S: Service\n",
        "\n",
        "# P: People\n",
        "\n",
        "# E: Empowerment\n",
        "\n",
        "# C: Caring\n",
        "\n",
        "# T: Teamwork\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPiCnk_FeBd3",
        "outputId": "2344ae4c-5c14-4a34-82b9-7b0d2c77448b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query: What does the acronym “RESPECT” in SPIL’s sales vision stand for?\n",
            "\n",
            "\n",
            "Answer:\n",
            "The acronym \"RESPECT\" in SPIL’s sales vision stands for:\n",
            "\n",
            "R: Reliability - You can count on us  \n",
            "E: Excellence - Is our Standard  \n",
            "S: Service - Customer First and accomplish the needs  \n",
            "P: People - Serve People with Fairness & Firmness  \n",
            "E: (Not specified in the provided text)  \n",
            "C: (Not specified in the provided text)  \n",
            "T: (Not specified in the provided text)  \n",
            "\n",
            "The details for E, C, and T are not provided in the information given.\n",
            "\n",
            "Retrieved Passages:\n",
            "--> To be one of the most respectable brands in the category through brand building initiatives, \n",
            "providing world class products with consistent, quality, leading to profitability and growth of \n",
            "everyone  ...\n",
            "--> innovative and cost saving solution within their total production process. \"Team Sirca” works had \n",
            "to understand their customer’s products and production processes to become their most reliable & \n",
            "dep ...\n",
            "--> SPIL Corporate HR Policies  \n",
            "  \n",
            "  \n",
            "  \n",
            " Respect of the core values, policies and procedures, manuals to achieve the goals of the \n",
            "organization.  \n",
            " \n",
            " Proper and maximum utilization of manpower and  ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_pdf_agent(\"What are the eligibility criteria and process for the Employee Children Merit Reward?\")\n",
        "\n",
        "# Q2:\n",
        "# What are the eligibility criteria and process for the Employee Children Merit Reward?\n",
        "\n",
        "# A2:\n",
        "\n",
        "# Based on marks obtained in school/college:\n",
        "\n",
        "# Class V: ₹2,500 if marks > 85%\n",
        "\n",
        "# Class VIII: ₹3,500 if marks > 80%\n",
        "\n",
        "# Class X: ₹5,000 if marks > 80%\n",
        "\n",
        "# Class XII: ₹10,000 if marks > 80%\n",
        "\n",
        "# Child’s name is published on the company website and HR newsletter."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNsOKaJaeGzs",
        "outputId": "f45de46f-9857-46db-9a8d-c55157556fff"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query: What are the eligibility criteria and process for the Employee Children Merit Reward?\n",
            "\n",
            "\n",
            "Answer:\n",
            "The eligibility criteria and process for the Employee Children Merit Award at Sirca Paints India Limited are as follows:\n",
            "\n",
            "**Eligibility Criteria:**\n",
            "1. The award is available to children of employees of Sirca Paints India Limited.\n",
            "2. The child must achieve a certain percentage of marks in their school examinations:\n",
            "   - Class V: The child must secure marks above 85%.\n",
            "   - Class VIII: The child must secure marks above 80%.\n",
            "   - Class X: The child must secure marks above 80%.\n",
            "   - Class XII: The child must secure marks above 80%.\n",
            "\n",
            "**Award Details:**\n",
            "- Class V: Rs 2,500 as a cash reward.\n",
            "- Class VIII: Rs 3,500 as a cash reward.\n",
            "- Class X: Rs 5,000 as a cash reward.\n",
            "- Class XII: Rs 10,000 as a cash reward.\n",
            "\n",
            "**Process:**\n",
            "- The specific process for applying for the reward is not detailed in the provided information. However, typically, such awards require submission of the student's academic records to the HR department for verification.\n",
            "- Once eligibility is confirmed, the cash reward is likely disbursed to the employee for their child's achievement.\n",
            "\n",
            "**Recognition:**\n",
            "- The names of the children who secure the awards will be published on the website of Sirca Paints India Limited and also mentioned in the HR Newsletter.\n",
            "\n",
            "This reward system is designed to motivate and recognize the academic achievements of employees' children, aligning with the company's vision of valuing educational excellence.\n",
            "\n",
            "Retrieved Passages:\n",
            "--> To make it operational, the employee has to submit his wedding card to the HR Department and \n",
            "then HR Representative will initiate the whole process.  The amount will be paid, subject to the \n",
            "deductio ...\n",
            "--> SPIL Corporate HR Policies  \n",
            " \n",
            " \n",
            "Class X: Rs 5000/ - will be given as a CASH REWARD, if the child will \n",
            "secured the marks above 80%  \n",
            " \n",
            "Class XII: Rs 10000/ - will be given as a CASH REWARD, if the ch ...\n",
            "--> Award will be introduced. This reward will be given to employee who secures  the highest \n",
            "aggregate marks in School/Colleges. The details of the award is given below  \n",
            " \n",
            "Class V: Rs 2500/ - will be gi ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_pdf_agent(\"If an employee takes leave on Saturday and Monday, how is Sunday treated in attendance?\")\n",
        "\n",
        "# Q:\n",
        "# If an employee takes leave on Saturday and Monday, how is Sunday treated in attendance?\n",
        "\n",
        "# A:\n",
        "# Sunday will also be counted as leave.\n",
        "\n",
        "# As per Section 6 (Attendance & Leave), if leave is taken on both Saturday and Monday, the Sunday in between is also treated as a leave."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hqP3bbSeQ6w",
        "outputId": "18288668-9d5b-4bf5-9478-50220d60557b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query: If an employee takes leave on Saturday and Monday, how is Sunday treated in attendance?\n",
            "\n",
            "\n",
            "Answer:\n",
            "If an employee takes leave on both Saturday and Monday, then Sunday will also be treated as a leave according to the corporate HR policies.\n",
            "\n",
            "Retrieved Passages:\n",
            "--> 1961. \n",
            " \n",
            "q) If the employee will take the continuous leave after festival then the festival will be considered \n",
            "as a Leave. \n",
            " \n",
            "r) Sunday will be treated as a leave, if the employee will take a leave o ...\n",
            "--> SPIL Corporate HR Policies  \n",
            " \n",
            " \n",
            " \n",
            "l) Marketing Department will submit the “Daily Time Report” (DTR) to their Reporting Officers \n",
            "and based upon their DTR the attendance will be marked. Reporting Offi ...\n",
            "--> a) Employees are required to register their attendance electronically while reporting to work and \n",
            "before leaving the office through Attendance Biometric installed in the office premises, if not \n",
            "done ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_pdf_agent(\"Can an employee claim compensatory leave for working on a public holiday and then club it with another leave on Monday?\")\n",
        "\n",
        "# Q:\n",
        "# Can an employee claim compensatory leave for working on a public holiday and then club it with another leave on Monday?\n",
        "\n",
        "# A:\n",
        "# No.\n",
        "\n",
        "# Compensatory leave cannot be clubbed with other leaves to create long leaves. For example, if Saturday is claimed as Compensatory Leave and Monday as normal leave, Comp Off will not be granted."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeOiAc1deZPB",
        "outputId": "f6cde4a6-f0eb-4c61-9339-04160b4a5923"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query: Can an employee claim compensatory leave for working on a public holiday and then club it with another leave on Monday?\n",
            "\n",
            "\n",
            "Answer:\n",
            "Yes, an employee can claim compensatory leave for working on a public holiday and then club it with another leave on Monday. According to the provided rules, an employee who works on a public holiday is eligible for compensatory leave. After obtaining this compensatory leave, the employee can use it in conjunction with other leaves, such as taking a leave on the following Monday. However, it's important for the employee to inform or get approval from their Reporting Officer and HR Department regarding the arrangement of their leaves.\n",
            "\n",
            "Retrieved Passages:\n",
            "--> 1961. \n",
            " \n",
            "q) If the employee will take the continuous leave after festival then the festival will be considered \n",
            "as a Leave. \n",
            " \n",
            "r) Sunday will be treated as a leave, if the employee will take a leave o ...\n",
            "--> Employee who work on Weekly off/Public Holiday due to some project or assignment emergency, \n",
            "can avail the Compensatory Leave. The compensatory leave will subject to the completion of 9 \n",
            "working hours ...\n",
            "--> Every Employee will take the approval o r provide the information to their Reporting Officer \n",
            "and HR Department regarding the working of the days above.  \n",
            " \n",
            "b) Employee who will on Sunday’s/Public Hol ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Happy Learning"
      ],
      "metadata": {
        "id": "TYRkq4JfepJr"
      }
    }
  ]
}