{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDt5EKeIdYdr"
   },
   "source": [
    "# __Hyperparameter Tuning__\n",
    "- Hyperparameter tuning is the process of systematically searching for the best combination of hyperparameter values for a machine learning model.\n",
    "- It involves selecting a subset of hyperparameters and exploring different values for each hyperparameter to find the configuration that optimizes the model's performance on a given dataset.\n",
    "\n",
    "Let's understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovC08-LtG9-v"
   },
   "source": [
    "## Steps to be followed:\n",
    "1. Import the required libraries\n",
    "2. Load the dataset and standardize it\n",
    "3. Build and Train a Basic Deep Learning Model\n",
    "4. Define the HyperModel\n",
    "5. Instantiate the Tuner and Perform Hyperparameter Tuning\n",
    "6. Evaluate Both Models Using Multiple Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHDFVGGzdYdv"
   },
   "source": [
    "### Step 1: Import the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow==2.17.0 in ./.local/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: scikeras==0.13.0 in ./.local/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: keras==3.2.0 in ./.local/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (14.0.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (22.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (2.28.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.56.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (2.17.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.26.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in ./.local/lib/python3.10/site-packages (from scikeras==0.13.0) (1.5.2)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/site-packages (from keras==3.2.0) (13.5.3)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/site-packages (from keras==3.2.0) (0.0.7)\n",
      "Requirement already satisfied: optree in ./.local/lib/python3.10/site-packages (from keras==3.2.0) (0.13.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.37.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2022.6.15)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras==0.13.0) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras==0.13.0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras==0.13.0) (3.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (2.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich->keras==3.2.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->keras==3.2.0) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras==3.2.0) (0.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.17.0 scikeras==0.13.0 keras==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22904,
     "status": "ok",
     "timestamp": 1719220342797,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "hfbZk64D8wSB",
    "outputId": "14121b15-dced-44b2-b8fa-e15196149e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in ./.local/lib/python3.10/site-packages (from keras-tuner) (3.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from keras-tuner) (22.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from keras-tuner) (2.28.1)\n",
      "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/site-packages (from keras-tuner) (1.0.4)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from keras->keras-tuner) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from keras->keras-tuner) (1.23.5)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/site-packages (from keras->keras-tuner) (13.5.3)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/site-packages (from keras->keras-tuner) (0.0.7)\n",
      "Requirement already satisfied: h5py in ./.local/lib/python3.10/site-packages (from keras->keras-tuner) (3.12.1)\n",
      "Requirement already satisfied: optree in ./.local/lib/python3.10/site-packages (from keras->keras-tuner) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in ./.local/lib/python3.10/site-packages (from keras->keras-tuner) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/site-packages (from requests->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->keras-tuner) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->keras-tuner) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->keras-tuner) (2022.6.15)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.local/lib/python3.10/site-packages (from optree->keras->keras-tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich->keras->keras-tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->keras->keras-tuner) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.1)\n",
      "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable oneDNN optimizations to avoid potential minor numerical differences caused by floating-point round-off errors.\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1719221527277,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "qHsqGlzNdYdw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 09:51:42.991612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-06 09:51:43.006384: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-06 09:51:43.010815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-06 09:51:43.022570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-06 09:51:45.547752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import keras_tuner\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lTAZH6jGVcO"
   },
   "source": [
    "### Step 2: Load the dataset and standardize it\n",
    "- In this code, the breast cancer dataset is loaded using the **load_breast_cancer** function.\n",
    "- The features are stored in the **X** variable, and the corresponding labels are stored in **y**.\n",
    "- Split the dataset into training and testing sets, then standardizes the training data to have zero mean and unit variance, applying the same transformation to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1719220365154,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "6JwDj1KddYdz"
   },
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_sc = scaler.fit_transform(X_train)\n",
    "test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRyQurw2jsMi"
   },
   "source": [
    "### Step 3: Build and Train a Basic Deep Learning Model\n",
    "\n",
    "- Constructs a sequential neural network with two hidden layers of 32 and 16 neurons respectively, using ReLU activation, and a dropout layer to reduce overfitting. The output layer uses a sigmoid activation function for binary classification.\n",
    "\n",
    "- Prepares the model for training by specifying the Adam optimizer, binary cross-entropy loss function for binary classification, and tracks the accuracy metric.\n",
    "\n",
    "- Fits the model on the standardized training data for 100 epochs, using 10% of it as a validation set to monitor performance, without verbosity to minimize output during training.\n",
    "\n",
    "- Assesses the model's performance on the standardized test data, obtaining the loss and accuracy, then prints the accuracy to give an indication of how well the model predicts unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12339,
     "status": "ok",
     "timestamp": 1719220387177,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "-k-7dw0udYd0",
    "outputId": "31cc39a5-eb8a-491b-cd8f-7d34ee40b18e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/voc/work/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-11-06 09:51:55.584137: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.1061 \n",
      "Basic Model Accuracy:  0.9736841917037964\n"
     ]
    }
   ],
   "source": [
    "# Build a basic model\n",
    "basic_model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(train_sc.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "basic_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "basic_model.fit(train_sc, y_train, epochs=100, validation_split=0.1, verbose=0)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "basic_loss, basic_accuracy = basic_model.evaluate(test_sc, y_test)\n",
    "print(\"Basic Model Accuracy: \", basic_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZsObONRki1-"
   },
   "source": [
    "### Step 4:  Define the HyperModel\n",
    "Defines a custom class MyHyperModel that extends the HyperModel class from Keras Tuner, used for hyperparameter tuning.\n",
    "\n",
    "- The class `MyHyperModel` is designed to construct a neural network model dynamically, with varying hyperparameters.\n",
    "- The `__init__` method initializes the class with an `input_shape`, which is the shape (number of features) of the input data that the model will expect. This is stored as a class attribute to be used later in building the model.\n",
    "\n",
    "- The build method creates a neural network model architecture with tunable hyperparameters.\n",
    "- `hp.Int('units', min_value=10, max_value=100, step=10)` This line specifies that the number of units in the Dense layers should be treated as a hyperparameter, with possible values ranging from 10 to 100 in steps of 10.\n",
    "\n",
    "- `hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)` This specifies that the dropout rate should also be a hyperparameter, ranging from 0.0 to 0.5 with a step of 0.1.\n",
    "\n",
    "- `model.add(Dense(1, activation='sigmoid'))` Adds an output layer with a single unit and sigmoid activation suitable for binary classification.\n",
    "\n",
    "- Learning rate for the Adam optimizer is configured with another tunable parameter `(hp.Float('learning_rate', ...))` which varies logarithmically from 0.0001 to 0.01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1719221150498,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "7EMzKZEJdYd1"
   },
   "outputs": [],
   "source": [
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(\n",
    "            units=hp.Int('units', min_value=10, max_value=100, step=10),\n",
    "            activation='relu', input_shape=(self.input_shape,)\n",
    "        ))\n",
    "        model.add(Dropout(\n",
    "            hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)\n",
    "        ))\n",
    "        model.add(Dense(\n",
    "            units=hp.Int('units', min_value=10, max_value=100, step=10),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHDK8vEbl9jU"
   },
   "source": [
    "### Step 5: Instantiate the Tuner and Perform Hyperparameter Tuning\n",
    "- Conducts hyperparameter tuning using Keras Tuner's RandomSearch, optimizing the neural network's configuration to maximize validation accuracy by testing different combinations of model parameters and identifying the best performing model.\n",
    "\n",
    "- It sets up a hyperparameter optimization process targeting the validation accuracy for a model defined by hypermodel.\n",
    "\n",
    "- The process will try up to 10 different sets of hyperparameters, running each configuration twice to ensure stability in the reported performance metrics, all within the specified project directory for organized storage and potential review.\n",
    "\n",
    "- This approach is useful for exploring a potentially vast hyperparameter space more efficiently than exhaustively testing all combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122907,
     "status": "ok",
     "timestamp": 1719221293117,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "QfeRXRDtdYd2",
    "outputId": "119699fc-ef07-4640-89f6-c5d91a65f161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Complete [00h 00m 06s]\n",
      "val_accuracy: 0.9835164844989777\n",
      "\n",
      "Best val_accuracy So Far: 0.9835164844989777\n",
      "Total elapsed time: 00h 00m 52s\n",
      "\n",
      "Search: Running Trial #9\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "20                |80                |units\n",
      "0.1               |0.3               |dropout\n",
      "0.00014531        |0.0080711         |learning_rate\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.4371 - loss: 0.7427 - val_accuracy: 0.5824 - val_loss: 0.6933\n",
      "Epoch 2/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4580 - loss: 0.7555 - val_accuracy: 0.5934 - val_loss: 0.6733\n",
      "Epoch 3/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5775 - loss: 0.7112 - val_accuracy: 0.6484 - val_loss: 0.6544\n",
      "Epoch 4/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6731 - loss: 0.6838 - val_accuracy: 0.6703 - val_loss: 0.6373\n",
      "Epoch 5/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6799 - loss: 0.6733 - val_accuracy: 0.6923 - val_loss: 0.6203\n",
      "Epoch 6/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7123 - loss: 0.6449 - val_accuracy: 0.7582 - val_loss: 0.6044\n",
      "Epoch 7/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7107 - loss: 0.6543 - val_accuracy: 0.7692 - val_loss: 0.5888\n",
      "Epoch 8/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7630 - loss: 0.6389 - val_accuracy: 0.7802 - val_loss: 0.5735\n",
      "Epoch 9/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8031 - loss: 0.5930 - val_accuracy: 0.8132 - val_loss: 0.5584\n",
      "Epoch 10/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7968 - loss: 0.5962 - val_accuracy: 0.8132 - val_loss: 0.5440\n",
      "Epoch 11/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8095 - loss: 0.5792 - val_accuracy: 0.8462 - val_loss: 0.5295\n",
      "Epoch 12/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8045 - loss: 0.5563 - val_accuracy: 0.8571 - val_loss: 0.5147\n",
      "Epoch 13/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8589 - loss: 0.5381 - val_accuracy: 0.8681 - val_loss: 0.5002\n",
      "Epoch 14/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8339 - loss: 0.5326 - val_accuracy: 0.8681 - val_loss: 0.4856\n",
      "Epoch 15/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8469 - loss: 0.5078 - val_accuracy: 0.8681 - val_loss: 0.4711\n",
      "Epoch 16/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8697 - loss: 0.4890 - val_accuracy: 0.9011 - val_loss: 0.4571\n",
      "Epoch 17/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8684 - loss: 0.4835 - val_accuracy: 0.9011 - val_loss: 0.4438\n",
      "Epoch 18/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8596 - loss: 0.4867 - val_accuracy: 0.9011 - val_loss: 0.4305\n",
      "Epoch 19/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8764 - loss: 0.4547 - val_accuracy: 0.9231 - val_loss: 0.4176\n",
      "Epoch 20/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8959 - loss: 0.4373 - val_accuracy: 0.9231 - val_loss: 0.4047\n",
      "Epoch 21/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8468 - loss: 0.4433 - val_accuracy: 0.9231 - val_loss: 0.3923\n",
      "Epoch 22/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8668 - loss: 0.4276 - val_accuracy: 0.9231 - val_loss: 0.3806\n",
      "Epoch 23/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8999 - loss: 0.4000 - val_accuracy: 0.9231 - val_loss: 0.3693\n",
      "Epoch 24/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8862 - loss: 0.3893 - val_accuracy: 0.9231 - val_loss: 0.3583\n",
      "Epoch 25/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8829 - loss: 0.3797 - val_accuracy: 0.9231 - val_loss: 0.3480\n",
      "Epoch 26/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9007 - loss: 0.3524 - val_accuracy: 0.9231 - val_loss: 0.3381\n",
      "Epoch 27/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9239 - loss: 0.3577 - val_accuracy: 0.9231 - val_loss: 0.3287\n",
      "Epoch 28/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9033 - loss: 0.3501 - val_accuracy: 0.9231 - val_loss: 0.3198\n",
      "Epoch 29/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9064 - loss: 0.3239 - val_accuracy: 0.9231 - val_loss: 0.3110\n",
      "Epoch 30/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9008 - loss: 0.3293 - val_accuracy: 0.9231 - val_loss: 0.3026\n",
      "Epoch 31/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8844 - loss: 0.3334 - val_accuracy: 0.9231 - val_loss: 0.2948\n",
      "Epoch 32/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9171 - loss: 0.3053 - val_accuracy: 0.9231 - val_loss: 0.2875\n",
      "Epoch 33/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9190 - loss: 0.2873 - val_accuracy: 0.9231 - val_loss: 0.2805\n",
      "Epoch 34/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9056 - loss: 0.3053 - val_accuracy: 0.9231 - val_loss: 0.2741\n",
      "Epoch 35/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9260 - loss: 0.2698 - val_accuracy: 0.9231 - val_loss: 0.2677\n",
      "Epoch 36/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9224 - loss: 0.2848 - val_accuracy: 0.9231 - val_loss: 0.2617\n",
      "Epoch 37/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9177 - loss: 0.2678 - val_accuracy: 0.9231 - val_loss: 0.2560\n",
      "Epoch 38/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9213 - loss: 0.2479 - val_accuracy: 0.9121 - val_loss: 0.2506\n",
      "Epoch 39/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9228 - loss: 0.2540 - val_accuracy: 0.9121 - val_loss: 0.2455\n",
      "Epoch 40/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8944 - loss: 0.2578 - val_accuracy: 0.9121 - val_loss: 0.2407\n",
      "Epoch 41/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9270 - loss: 0.2326 - val_accuracy: 0.9121 - val_loss: 0.2360\n",
      "Epoch 42/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9118 - loss: 0.2633 - val_accuracy: 0.9121 - val_loss: 0.2315\n",
      "Epoch 43/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9099 - loss: 0.2446 - val_accuracy: 0.9121 - val_loss: 0.2273\n",
      "Epoch 44/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8939 - loss: 0.2593 - val_accuracy: 0.9121 - val_loss: 0.2232\n",
      "Epoch 45/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9332 - loss: 0.2254 - val_accuracy: 0.9121 - val_loss: 0.2191\n",
      "Epoch 46/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9339 - loss: 0.2174 - val_accuracy: 0.9121 - val_loss: 0.2153\n",
      "Epoch 47/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9208 - loss: 0.2431 - val_accuracy: 0.9121 - val_loss: 0.2117\n",
      "Epoch 48/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9188 - loss: 0.2162 - val_accuracy: 0.9121 - val_loss: 0.2083\n",
      "Epoch 49/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9407 - loss: 0.2016 - val_accuracy: 0.9121 - val_loss: 0.2050\n",
      "Epoch 50/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9402 - loss: 0.2131 - val_accuracy: 0.9121 - val_loss: 0.2019\n",
      "Epoch 1/50\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 675ms/step - accuracy: 0.7188 - loss: 0.6512"
     ]
    }
   ],
   "source": [
    "# Assuming 'train_sc' and 'y_train' are defined as your scaled training data and labels\n",
    "input_shape = train_sc.shape[1]  # Extract the number of features\n",
    "\n",
    "# Create an instance of the HyperModel\n",
    "hypermodel = MyHyperModel(input_shape=input_shape)\n",
    "\n",
    "# Instantiate the tuner\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='tuner_data',\n",
    "    project_name='breast_cancer_optimization'\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "tuner.search(train_sc, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1qQrUoWK0Xr"
   },
   "source": [
    "#### **Observation:**\n",
    "\n",
    "- Trial 10 Complete: Indicates the completion of the 10th trial in the hyperparameter tuning process.\n",
    "\n",
    "- val_accuracy: Indicates the validation accuracy achieved by the model configuration tested in the 10th trial. It indicates how well the model performed on the validation set, which is a subset of the training data not used for training the model but for evaluating its performance.\n",
    "\n",
    "- Best val_accuracy So Far: 0.9890109896659851: Up to this point in the tuning process, this is the highest validation accuracy achieved across all trials. This suggests that the model configuration from the 10th trial is currently the best performer among all configurations tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3w1EHOEnLWd"
   },
   "source": [
    "### Step 6: : Evaluate Both Models Using Multiple Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1719221536717,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "ef-uEPxAdYd2",
    "outputId": "dfc59575-b457-4d6d-e60b-039e9775eeee"
   },
   "outputs": [],
   "source": [
    "# Predict probabilities for the basic and best models\n",
    "basic_predictions_proba = basic_model.predict(test_sc)\n",
    "basic_predictions = (basic_predictions_proba > 0.5).astype(int)\n",
    "\n",
    "best_predictions_proba = best_model.predict(test_sc)\n",
    "best_predictions = (best_predictions_proba > 0.5).astype(int)\n",
    "\n",
    "# Print classification report for basic model\n",
    "print(\"Basic Model Classification Report:\")\n",
    "print(classification_report(y_test, basic_predictions, target_names=['Benign', 'Malignant']))\n",
    "\n",
    "# Calculate and print ROC AUC for the basic model\n",
    "basic_auc = roc_auc_score(y_test, basic_predictions_proba)\n",
    "print(\"Basic Model ROC AUC:\", basic_auc)\n",
    "\n",
    "# Print classification report for best model\n",
    "print(\"Best Model Classification Report:\")\n",
    "print(classification_report(y_test, best_predictions, target_names=['Benign', 'Malignant']))\n",
    "\n",
    "# Calculate and print ROC AUC for the best model\n",
    "best_auc = roc_auc_score(y_test, best_predictions_proba)\n",
    "print(\"Best Model ROC AUC:\", best_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98VeSPHzqMdK"
   },
   "source": [
    "#### **Observation:**\n",
    "\n",
    "**Basic Model Results:**\n",
    "\n",
    "- Classification Report:\n",
    "    - Benign: Precision of 0.98, recall of 0.95, and F1-score of 0.96. High precision and recall suggest the model effectively identifies benign cases.\n",
    "    - Malignant: Precision of 0.97, recall of 1, and F1-score of 0.98. This shows excellent performance in identifying malignant cases with very high precision.\n",
    "    - Overall Accuracy: 97%, indicating a high overall rate of correct predictions.\n",
    "- ROC AUC: 0.99, which is very close to 1. This score indicates an excellent ability to discriminate between the benign and malignant classes.\n",
    "\n",
    "**Best Model Results:**\n",
    "\n",
    "- Classification Report:\n",
    "    - Benign: Perfect precision of 1.00 and recall of 0.95, resulting in an F1-score of 0.98.\n",
    "    - Malignant: Precision of 0.97 and a perfect recall of 1.00, with an F1-score of 0.99. This configuration slightly improves in identifying all malignant cases compared to the basic model.\n",
    "    - Overall Accuracy: 98%, a slight improvement over the basic model.\n",
    "- ROC AUC: 0.99, indicating a marginal but notable improvement in discriminative ability over the basic model.\n",
    "\n",
    "As per the results, the best model, which underwent hyperparameter tuning, shows improved performance metrics across most areas compared to the basic model. While both models perform exceptionally well, the best model's perfect recall for malignant cases and higher overall ROC AUC suggest it is slightly more reliable, particularly in scenarios where failing to detect malignant cases (false negatives) is critically risky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
