{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4CNzLHQWbPg"
   },
   "source": [
    "#__Video Classification Using Hybrid Model__\n",
    "Let's see how to classify the video using transfer learning and a recurrent model on the UCF101 dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktbWX3ddWxa9"
   },
   "source": [
    "## Steps to be followed:\n",
    "1. Download data and import the required libraries\n",
    "2. Read the data from datasets and print the ten rows\n",
    "3. Define the functions for cropping and loading video frames\n",
    "4. Build a feature extraction model using the InceptionV3 architecture\n",
    "5. Create a string lookup table for labels and print the vocabulary of the label processor\n",
    "6. Prepare video data for training and testing by extracting frame features\n",
    "7. Define and train a sequence model using GRU layers\n",
    "8. Load a test video, extract frame features, and make predictions using the sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvjlPz3dW5Bs"
   },
   "source": [
    "### Step 1: Download data and import the required libraries\n",
    "- Download the dataset.\n",
    "- Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow==2.17.0\n",
      "  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting scikeras==0.13.0\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting keras==3.2.0\n",
      "  Downloading keras-3.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.6.3)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow==2.17.0)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.2.0)\n",
      "Collecting h5py>=3.10.0 (from tensorflow==2.17.0)\n",
      "  Downloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (14.0.1)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow==2.17.0)\n",
      "  Downloading ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (22.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (2.28.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.56.2)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow==2.17.0)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.26.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.23.5)\n",
      "Collecting scikit-learn>=1.4.2 (from scikeras==0.13.0)\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/site-packages (from keras==3.2.0) (13.5.3)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/site-packages (from keras==3.2.0) (0.0.7)\n",
      "Collecting optree (from keras==3.2.0)\n",
      "  Downloading optree-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m737.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.37.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2022.6.15)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras==0.13.0) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras==0.13.0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras==0.13.0) (3.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (2.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich->keras==3.2.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->keras==3.2.0) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras==3.2.0) (0.1.1)\n",
      "Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Downloading keras-3.2.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading optree-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: flatbuffers, tensorboard, optree, ml-dtypes, h5py, scikit-learn, keras, tensorflow, scikeras\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/voc/work/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/voc/work/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-compression 2.13.0 requires tensorflow~=2.13.0, but you have tensorflow 2.17.0 which is incompatible.\n",
      "tensorflow-federated 0.63.0 requires tensorflow==2.13.*,>=2.13.0, but you have tensorflow 2.17.0 which is incompatible.\n",
      "tensorflow-text 2.13.0 requires tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.17.0 which is incompatible.\n",
      "tf-models-official 2.13.2 requires tensorflow~=2.13.0, but you have tensorflow 2.17.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed flatbuffers-24.3.25 h5py-3.12.1 keras-3.2.0 ml-dtypes-0.4.1 optree-0.13.0 scikeras-0.13.0 scikit-learn-1.5.2 tensorboard-2.17.1 tensorflow-2.17.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.17.0 scikeras==0.13.0 keras==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imutils\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25858 sha256=525c4ce80cea8015fce56ead31c2a124e387fbddabacc3f2732027a6712dacbb\n",
      "  Stored in directory: /voc/work/.cache/pip/wheels/85/cf/3a/e265e975a1e7c7e54eb3692d6aa4e2e7d6a3945d29da46f2d7\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable oneDNN optimizations to avoid potential minor numerical differences caused by floating-point round-off errors.\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 138069,
     "status": "ok",
     "timestamp": 1718963614653,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "0t8MHucxPGBU"
   },
   "outputs": [],
   "source": [
    "!wget -q --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\n",
    "!wget -q --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 122337,
     "status": "ok",
     "timestamp": 1718963762533,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "FufAi4ZnPJ7S"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unrar e UCF101.rar data/\n",
    "!unzip -qq UCF101TrainTestSplits-RecognitionTask.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11065,
     "status": "ok",
     "timestamp": 1718963781963,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "6DT8IxSDPJ-U"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdj2UXjiFr0h"
   },
   "source": [
    "Open the __.txt__ file which has the names of the training videos.\n",
    "\n",
    "Create a DataFrame having video names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1718963791258,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "BoFiu5HpPKBt",
    "outputId": "72991c98-0933-4598-d827-ff985c95ef31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      video_name\n",
       "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n",
       "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1\n",
       "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1\n",
       "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1\n",
       "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"ucfTrainTestlist/trainlist01.txt\", \"r\")\n",
    "temp = f.read()\n",
    "videos = temp.split('\\n')\n",
    "\n",
    "train = pd.DataFrame()\n",
    "train['video_name'] = videos\n",
    "train = train[:-1]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1F6_vxikF3zd"
   },
   "source": [
    "Open the __.txt__ file which has the names of the test videos.\n",
    "\n",
    "Create a DataFrame having video names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1718963801435,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "fGH6nO23PKFO",
    "outputId": "e1acaa61-72e6-47f5-d9d7-f57653a7224d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    video_name\n",
       "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\n",
       "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi\n",
       "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi\n",
       "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi\n",
       "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"ucfTrainTestlist/testlist01.txt\", \"r\") as f:\n",
    "    temp = f.read()\n",
    "videos = temp.split(\"\\n\")\n",
    "\n",
    "test = pd.DataFrame()\n",
    "test[\"video_name\"] = videos\n",
    "test = test[:-1]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1718963925073,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "ji937vL-OlIZ",
    "outputId": "3b5408e6-41aa-4565-94b5-d346e68cb94f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data:  9537\n",
      "Length of test data:  3783\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train data: \", len(train))\n",
    "print(\"Length of test data: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwJeWtfLHQs-"
   },
   "source": [
    "- Define the __extract_tag__ function that extracts a tag from the video path. This is done by splitting the video path by '/' and returning the first part.\n",
    "- Define the __separate_video_name__ function, which separates the video name from the video path. This is achieved by splitting the video name by / and returning the second part.\n",
    "- Define the __rectify_video_name__ function to rectify the video name by splitting the video name by \" \" and returning the first part.\n",
    "- Define the __move_videos__ function:\n",
    "   - Check if the output directory exists. If not, create the directory using __os.mkdir__.\n",
    "   - Iterate over the DataFrame, __df__, using a progress bar from the __tqdm__ library.\n",
    "   - For each row in the DataFrame, extract the video file name from the __video_name__ column, create its path, and then copy the video file to the output directory using __shutil.copy2__.\n",
    "   - After the loop ends, print the total number of videos in the output directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1718964134452,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "yi9ooC9XPfYe"
   },
   "outputs": [],
   "source": [
    "def extract_tag(video_path):\n",
    "    return video_path.split(\"/\")[0]\n",
    "\n",
    "def separate_video_name(video_name):\n",
    "    return video_name.split(\"/\")[1]\n",
    "\n",
    "def rectify_video_name(video_name):\n",
    "    return video_name.split(\" \")[0]\n",
    "\n",
    "def move_videos(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        videoFile = df['video_name'][i].split(\"/\")[-1]\n",
    "        videoPath = os.path.join(\"data\", videoFile)\n",
    "        shutil.copy2(videoPath, output_dir)\n",
    "    print()\n",
    "    print(f\"Total videos: {len(os.listdir(output_dir))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55J4781CXO9b"
   },
   "source": [
    "### Step 2: Read the data from datasets and print the ten rows\n",
    "\n",
    "- Define the values of parameters **IMG_SIZE**, **BATCH_SIZE**, **EPOCHS**, **MAX_SEQ_LENGTH**, and **NUM_FEATURES.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1718964164120,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "UwjxIY0xPlw7"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUlUP1OOXWc4"
   },
   "source": [
    "### Step 3: Define the functions for cropping and loading video frames\n",
    "- Define a function named **crop_center_square** that crops a frame to a square shape by determining the minimum dimension and calculating the starting coordinates.\n",
    "- Define a function named __load_video__ that loads a video file, crops each frame to a square shape, resizes it, and converts the color channels.\n",
    "- Open the video file using **cv2.VideoCapture** and initialize an empty list called frames.\n",
    "- Read frames from the video, crop them to a square shape, resize them, convert the color channels, and append them to the frames list.\n",
    "- If the maximum number of frames is reached or the video ends, exit the loop.\n",
    "- Release the video capture.\n",
    "- Convert the frame list to a NumPy array.\n",
    "- Return the array of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1718964271597,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "_HzfusTJPpPV"
   },
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j2lXMm4Xd8V"
   },
   "source": [
    "__Observation:__\n",
    "- The code defines two functions, **crop_center_square** and __load_video__, which can be used to crop frames from videos and load videos as arrays of frames, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14OJjHbUXjrh"
   },
   "source": [
    "### Step 4: Build a feature extraction model using InceptionV3 architecture\n",
    "\n",
    "- Create a feature extractor using the InceptionV3 model from keras.applications with specific configurations.\n",
    "- Assign the preprocess_input function from **keras.applications.inception_v3** to the **variable preprocess_input**.\n",
    "- Create an input layer with the shape __(IMG_SIZE, IMG_SIZE, 3)__ using **keras.Input**.\n",
    "- Preprocess the input using the **preprocess_input** function.\n",
    "- Pass the preprocessed input through the feature extractor to obtain the outputs.\n",
    "- Create a model with the inputs and outputs using **keras.Model** and assign it to the variable **feature_extractor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7714,
     "status": "ok",
     "timestamp": 1718964454741,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "Rjl2yoPmPtoW",
    "outputId": "0a75e2c2-21bb-49aa-944f-680207038454"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 06:02:44.710611: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rN_SBAOYG8B"
   },
   "source": [
    "__Observations:__\n",
    "- The code defines a function **build_feature_extractor** that creates a feature extractor model using InceptionV3 architecture.\n",
    "- The model takes inputs of size __(IMG_SIZE, IMG_SIZE, 3)__, preprocesses the inputs, and produces the outputs.\n",
    "- The created feature extractor model is assigned to the variable **feature_extractor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1718964529638,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "XMkKyQQ6QCMV",
    "outputId": "c073adc1-8070-4fca-bbfa-4c0b3891b4a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c01.avi 1</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c02.avi 1</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c03.avi 1</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c04.avi 1</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c05.avi 1</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       video_name             tag\n",
       "0  v_ApplyEyeMakeup_g08_c01.avi 1  ApplyEyeMakeup\n",
       "1  v_ApplyEyeMakeup_g08_c02.avi 1  ApplyEyeMakeup\n",
       "2  v_ApplyEyeMakeup_g08_c03.avi 1  ApplyEyeMakeup\n",
       "3  v_ApplyEyeMakeup_g08_c04.avi 1  ApplyEyeMakeup\n",
       "4  v_ApplyEyeMakeup_g08_c05.avi 1  ApplyEyeMakeup"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"tag\"] = train[\"video_name\"].apply(extract_tag)\n",
    "train[\"video_name\"] = train[\"video_name\"].apply(separate_video_name)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1718964582524,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "ialV1htPQGlg",
    "outputId": "54031495-6751-4533-c8d2-cad7ffd99a44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c01.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c02.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c03.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c04.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c05.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     video_name             tag\n",
       "0  v_ApplyEyeMakeup_g08_c01.avi  ApplyEyeMakeup\n",
       "1  v_ApplyEyeMakeup_g08_c02.avi  ApplyEyeMakeup\n",
       "2  v_ApplyEyeMakeup_g08_c03.avi  ApplyEyeMakeup\n",
       "3  v_ApplyEyeMakeup_g08_c04.avi  ApplyEyeMakeup\n",
       "4  v_ApplyEyeMakeup_g08_c05.avi  ApplyEyeMakeup"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"video_name\"] = train[\"video_name\"].apply(rectify_video_name)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1173,
     "status": "ok",
     "timestamp": 1718964667861,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "a07ssIhrQL_H",
    "outputId": "7a65a436-9026-4680-fad5-1397678df816"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c01.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c02.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c03.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c04.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c05.avi</td>\n",
       "      <td>ApplyEyeMakeup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     video_name             tag\n",
       "0  v_ApplyEyeMakeup_g01_c01.avi  ApplyEyeMakeup\n",
       "1  v_ApplyEyeMakeup_g01_c02.avi  ApplyEyeMakeup\n",
       "2  v_ApplyEyeMakeup_g01_c03.avi  ApplyEyeMakeup\n",
       "3  v_ApplyEyeMakeup_g01_c04.avi  ApplyEyeMakeup\n",
       "4  v_ApplyEyeMakeup_g01_c05.avi  ApplyEyeMakeup"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"tag\"] = test[\"video_name\"].apply(extract_tag)\n",
    "test[\"video_name\"] = test[\"video_name\"].apply(separate_video_name)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYddLnv3V2z3"
   },
   "source": [
    "**Filter Top-N Activities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1718964843471,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "G6zxubqKQQav",
    "outputId": "e0939a51-0ec0-4bed-cfed-9bd6d195d255"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 2), (0, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "topNActs = train[\"tag\"].value_counts().nlargest(n).reset_index()[\"tag\"].tolist()\n",
    "train_new = train[train[\"tag\"].isin(topNActs)]\n",
    "test_new = test[test[\"tag\"].isin(topNActs)]\n",
    "train_new.shape, test_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Q2oQQC5KQZc"
   },
   "source": [
    "**Observation:**\n",
    "- The output **((1171, 2), (459, 2))** is a tuple showing the shapes of **train_new** and **test_new**. The **train_new** DataFrame has 1171 rows and 2 columns, and the **test_new** DataFrame has 459 rows and 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1718964947263,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "mBPvTP0RQUGb"
   },
   "outputs": [],
   "source": [
    "train_new = train_new.reset_index(drop=True)\n",
    "test_new = test_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNw7O0LdXzFV"
   },
   "source": [
    "### Step 5: Create a string lookup table for labels and print the vocabulary of the label processor\n",
    "- Create a label processor using **keras.layers.StringLookup.**\n",
    "- Set the number of out-of-vocabulary (OOV) indices to **0.**\n",
    "- Set the vocabulary of the label processor to the unique values from the **tag** column of the **train_df** DataFrame.\n",
    "- Retrieve the vocabulary of the label processor using **label_processor.get_vocabulary().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1718964981436,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "tHg5adMbTlyr",
    "outputId": "bce6c49b-aeee-434a-bc19-5c3744083fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', 'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', 'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'HammerThrow', 'Hammering', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', 'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', 'JugglingBalls', 'JumpRope', 'JumpingJack', 'Kayaking', 'Knitting', 'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', 'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', 'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', 'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', 'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', 'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', 'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', 'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', 'YoYo']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train[\"tag\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF_te3XFYQku"
   },
   "source": [
    "__Observations:__\n",
    "- The code creates a label processor that maps labels from text to integer indices.\n",
    "- It uses the unique values from the **tag** column of the **train** DataFrame as the vocabulary for the label processor.\n",
    "- The output is the vocabulary of the label processor, which is a list of unique labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsh5ULu7YU36"
   },
   "source": [
    "### Step 6: Prepare video data for training and testing by extracting frame features\n",
    "- Define a function named **prepare_all_videos** that inputs a DataFrame (df) and a root directory **(root_dir)**.\n",
    "- Retrieve the video paths and labels from the DataFrame and encode the labels using **label_processor**.\n",
    "- Initialize arrays to store frame masks and frame features for each video.\n",
    "- Iterate over each video in the dataset, load the frames, and extract features using the **feature_extractor** model.\n",
    "- Update the arrays with the extracted features and masks for each video.\n",
    "- Call the **prepare_all_videos** function on the train and test DataFrames, storing the returned values in **train_data**, **train_labels**, **test_data**, and __test_labels__.\n",
    "- Finally, print the shape of the frame features in the train set and the shape of the frame masks in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4874,
     "status": "ok",
     "timestamp": 1718965066800,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "ARvHX2VATt36",
    "outputId": "29c1fbf6-2883-42e0-f86e-aa8a087c5552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (9537, 20, 2048)\n",
      "Frame masks in train set: (9537, 20)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohCZ1_qtYa8e"
   },
   "source": [
    "__Observations:__\n",
    "- The code processes videos by extracting frame features and creating frame masks.\n",
    "- It then returns the frame features, frame masks, and labels for the train and test sets.\n",
    "- The output is the shape of the frame features in the train set and the shape of the frame masks in the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztRcM2knYgPA"
   },
   "source": [
    "### Step 7: Define and train a sequence model using GRU layers\n",
    "- Define a function named **get_sequence_model** that creates a sequence model for video classification.\n",
    "- Create input layers for frame features and masks.\n",
    "- Apply two GRU layers to the frame features input, with the second GRU layer returning only the last output.\n",
    "- Add a dropout layer, a dense layer with ReLU activation, and a final dense layer with softmax activation for the output.\n",
    "- Compile the model with sparse categorical cross-entropy loss, Adam optimizer, and accuracy metric.\n",
    "- Define a function named __run_experiment__ for running the training and evaluation.\n",
    "- Set up a checkpoint to save the best model during training.\n",
    "- Create the sequence model using **get_sequence_model.**\n",
    "- Train the model on the training data with a validation split, a specified number of epochs, and the checkpoint callback.\n",
    "- Load the best weights saved during training.\n",
    "- Evaluate the model on the test data and print the test accuracy.\n",
    "- Return the history object and the trained sequence model.\n",
    "- Call the **run_experiment** function and store the returned values in __(history)__ and **sequence_model.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42937,
     "status": "ok",
     "timestamp": 1718965177073,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "KAgMUDWKT1Ba",
    "outputId": "7f05b043-a4e5-4a8f-880d-f3de7a8c6a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m207/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0163 - loss: 4.5974\n",
      "Epoch 1: val_loss improved from inf to 4.78483, saving model to /tmp/video_classifier.weights.h5\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 29ms/step - accuracy: 0.0163 - loss: 4.5971 - val_accuracy: 0.0000e+00 - val_loss: 4.7848\n",
      "Epoch 2/2\n",
      "\u001b[1m206/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0163 - loss: 4.5321\n",
      "Epoch 2: val_loss did not improve from 4.78483\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.0163 - loss: 4.5318 - val_accuracy: 0.0000e+00 - val_loss: 4.9505\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.0064 - loss: 4.5584\n",
      "Test accuracy: 1.16%\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifier.weights.h5\"  \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdIBd6N_Ym1z"
   },
   "source": [
    "__Observations:__\n",
    "- Training progress and validation metrics will be displayed during the model training process.\n",
    "- After training, the model will be evaluated on the test data, and the test accuracy will be printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZShMzB7Yyqp"
   },
   "source": [
    "### Step 8: Load a test video, extract frame features, and make predictions using the sequence model\n",
    "- Load a random test video path.\n",
    "- Call the **sequence_prediction** function with the test video path.\n",
    "- Within the **sequence_prediction** function:\n",
    "\n",
    "  a. Get the vocabulary of the classes.\n",
    "\n",
    "  b. Load the frames of the video.\n",
    "\n",
    "  c. Prepare the frames for sequence prediction by extracting frame features and creating a frame mask.\n",
    "\n",
    "  d. Use the trained sequence model to predict the probabilities of each class in the video.\n",
    "\n",
    "  e. Print the predicted class probabilities in descending order.\n",
    "\n",
    "- Assign the frames of the test video to the variable test_frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1718965783276,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "xezv1Nq3UDoz"
   },
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1718965830826,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "A49jDUlsWFcJ",
    "outputId": "c4d069d3-f420-46b2-94d3-2dd1a75b2f5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: v_JumpRope_g05_c05.avi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step\n",
      "  PlayingCello:  1.09%\n",
      "  HorseRiding:  1.09%\n",
      "  Drumming:  1.09%\n",
      "  CricketShot:  1.09%\n",
      "  BoxingPunchingBag:  1.08%\n",
      "  PlayingDaf:  1.08%\n",
      "  IceDancing:  1.08%\n",
      "  BandMarching:  1.08%\n",
      "  Kayaking:  1.08%\n",
      "  BenchPress:  1.08%\n",
      "  PlayingGuitar:  1.08%\n",
      "  PoleVault:  1.08%\n",
      "  PlayingDhol:  1.08%\n",
      "  PlayingSitar:  1.08%\n",
      "  PlayingFlute:  1.07%\n",
      "  ApplyEyeMakeup:  1.07%\n",
      "  Diving:  1.07%\n",
      "  Bowling:  1.07%\n",
      "  Billiards:  1.07%\n",
      "  Hammering:  1.07%\n",
      "  BaseballPitch:  1.07%\n",
      "  HeadMassage:  1.07%\n",
      "  Archery:  1.07%\n",
      "  HandstandPushups:  1.07%\n",
      "  JumpRope:  1.07%\n",
      "  CliffDiving:  1.07%\n",
      "  FrontCrawl:  1.07%\n",
      "  BoxingSpeedBag:  1.07%\n",
      "  GolfSwing:  1.07%\n",
      "  BasketballDunk:  1.07%\n",
      "  CricketBowling:  1.06%\n",
      "  HammerThrow:  1.06%\n",
      "  Basketball:  1.06%\n",
      "  Nunchucks:  1.06%\n",
      "  BabyCrawling:  1.06%\n",
      "  Biking:  1.06%\n",
      "  MilitaryParade:  1.06%\n",
      "  Haircut:  1.06%\n",
      "  BrushingTeeth:  1.06%\n",
      "  HulaHoop:  1.06%\n",
      "  BlowDryHair:  1.06%\n",
      "  Mixing:  1.06%\n",
      "  FloorGymnastics:  1.05%\n",
      "  Lunges:  1.05%\n",
      "  FrisbeeCatch:  1.05%\n",
      "  LongJump:  1.05%\n",
      "  Knitting:  1.05%\n",
      "  HighJump:  1.05%\n",
      "  FieldHockeyPenalty:  1.05%\n",
      "  JumpingJack:  1.05%\n",
      "  JavelinThrow:  1.05%\n",
      "  PizzaTossing:  1.05%\n",
      "  JugglingBalls:  1.05%\n",
      "  PommelHorse:  1.05%\n",
      "  BodyWeightSquats:  1.04%\n",
      "  PlayingPiano:  1.04%\n",
      "  CuttingInKitchen:  1.04%\n",
      "  HorseRace:  1.04%\n",
      "  PlayingTabla:  1.04%\n",
      "  CleanAndJerk:  1.04%\n",
      "  HandstandWalking:  1.04%\n",
      "  ParallelBars:  1.03%\n",
      "  ApplyLipstick:  1.03%\n",
      "  BalanceBeam:  1.03%\n",
      "  Fencing:  1.03%\n",
      "  BlowingCandles:  1.03%\n",
      "  MoppingFloor:  1.03%\n",
      "  PlayingViolin:  1.02%\n",
      "  BreastStroke:  1.02%\n",
      "  PullUps:  1.00%\n",
      "  Rowing:  0.84%\n",
      "  TrampolineJumping:  0.84%\n",
      "  TableTennisShot:  0.84%\n",
      "  TaiChi:  0.84%\n",
      "  TennisSwing:  0.84%\n",
      "  ThrowDiscus:  0.84%\n",
      "  Typing:  0.84%\n",
      "  Swing:  0.84%\n",
      "  UnevenBars:  0.84%\n",
      "  VolleyballSpiking:  0.84%\n",
      "  WalkingWithDog:  0.84%\n",
      "  WallPushups:  0.84%\n",
      "  WritingOnBoard:  0.84%\n",
      "  Punch:  0.84%\n",
      "  Surfing:  0.84%\n",
      "  SalsaSpin:  0.84%\n",
      "  SumoWrestling:  0.84%\n",
      "  StillRings:  0.84%\n",
      "  PushUps:  0.84%\n",
      "  SoccerPenalty:  0.84%\n",
      "  SoccerJuggling:  0.84%\n",
      "  SkyDiving:  0.84%\n",
      "  Rafting:  0.84%\n",
      "  Skijet:  0.84%\n",
      "  Skiing:  0.84%\n",
      "  RockClimbingIndoor:  0.84%\n",
      "  RopeClimbing:  0.84%\n",
      "  SkateBoarding:  0.84%\n",
      "  Shotput:  0.84%\n",
      "  ShavingBeard:  0.84%\n",
      "  YoYo:  0.84%\n"
     ]
    }
   ],
   "source": [
    "test_video = np.random.choice(test[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a01It9XpY46J"
   },
   "source": [
    "__Observations:__\n",
    "- The test video path will be printed.\n",
    "- The predicted class probabilities for the test video will be printed, showing the class label and the corresponding probability.\n",
    "- The frames of the test video will be assigned to the **test_frames** variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
