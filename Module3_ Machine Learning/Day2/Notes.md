6th April
--------
Model :

Classification Algo:
Regression Alog: 

simple leanear equation
	regression, We do it on infinite data. 
	Prediting a continueus vareable
	
	y = mx+c , simple leanear equation
	
multiple leanear regression
	when we have more leanear equations dependent on each others.. 
	y = m1xX1 +  m2xX2 + ..... c Multiple linear equation,

	
	Regressoin works on two things
	1. the independents are trully independent (not related to each other)
	2.  
	
----------------
Ordinary least square : to identify best fit values	, find out best fit line which is as close as to each points.. (+ , -) 	
	
R-squared: Increases when you add more variables, even if they are not useful.
Adjusted R-squared: Only increases if the new variable improves the model significantly.	
	
	
	
	
	
error value : Actual - prediction value
	
	
	
//  we cant use Lineal regression if there is a strong correlation between more than 2 features?



7th April : rivision
y = f(x1,x2,x3,...)


--------------

Supervised leaning version:

Standard scaling
Overfiting : model is generalized the data , error on test set is quite high.but tranning error is high
	RMSE of test is high.. and rmse of train set is low
	
	
Underfitted : model have not learnt any ting from data/


How to Overcome Overfitting: 
Lasso
ridge
Elastic net regression	(Rss +L1+ L2 is elasticnet.)
	
	
	
	
	
	
	
	